professor b: U u u u  , I meant , you know , this end of the world , yeah , is really what I meant , phd c: Oh . phd c: I did some experim  , just a few more experiments before I had to , go away for the w well , that week . professor b: Great ! phd c: Was it last week or whenever ?  , so what I was started playing with was the th again , this is the HTK back - end . And , I was curious because the way that they train up the models , they go through about four sort of rounds of of training . And in the first round they do  , I think it 's three iterations , and for the last three rounds e e they do seven iterations of re - estimation in each of those three . And so , you know , that 's part of what takes so long to train the the the back - end for this . It 's this program called H E professor b: But in HTK , what 's the difference between , a an inner loop and an outer loop in these iterations ? phd c: OK . So what happens is , at each one of these points , you increase the number of Gaussians in the model . phd c: And so , in the final one here , you end up with ,  for all of the the digit words , you end up with , three mixtures per state , professor b: Yeah . So I had done some experiments where I was I I want to play with the number of mixtures . phd c: But , I wanted to first test to see if we actually need to do this many iterations early on . phd c: And so , I I ran a couple of experiments where I reduced that to l to be three , two , two , five , I think , and I got almost the exact same results . So , I I think m it only took something like , three or four hours to do the full training , professor b: As opposed to ? phd f: Good . phd c: as opposed to wh what , sixteen hours or something like that ?  , it takes you have to do an overnight basically , the way it is set up now . phd c: So , even we don't do anything else , doing something like this could allow us to turn experiments around a lot faster . professor b: And then when you have your final thing , do a full one , so it 's phd c: And when you have your final thing , we go back to this .  , it 's like one little text file you edit and change those numbers , and you don't do anything else . phd c: So it 's a very simple change to make and it doesn't seem to hurt all that much . phd a: So you you run with three , two , two , five ? That 's a phd c: So I  , I I have to look to see what the exact numbers were . phd c: I I thought was , like , three , two , two , five , phd a:   Oh , the other thing that I did was , I compiled the HTK stuff for the Linux boxes . So , you can now run your experiments on that machine and you can run five at a time and it runs , as fast as , you know , five different machines . phd c: So , I 've forgotten now what the name of that machine is but I can I can send email around about it . phd c: And so we 've got it now HTK 's compiled for both the Linux and for , the Sparcs .  , you have to make you have to make sure that in your dot CSHRC , it detects whether you 're running on the Linux or a a Sparc and points to the right executables .  , and you may not have had that in your dot CSHRC before , if you were always just running the Sparc . phd c: I can I can tell you exactly what you need to do to get all of that to work . phd c: So , together with the fact that we 've got these faster Linux boxes and that it takes less time to do these , we should be able to crank through a lot more experiments . phd c: So after I did that , then what I wanted to do was try increasing the number of mixtures , just to see ,  see how how that affects performance . In fact , you could do something like keep exactly the same procedure and then add a fifth thing onto it phd c:   grad e: So at at the middle o where the arrows are showing , that 's you 're adding one more mixture per state , phd c:  - huh . It goes from this  , try to go it backwards this at this point it 's two mixtures per state . And I think what happens here is professor b: Might be between , shared ,  shared variances or something , phd c: Yeah . phd c: there because they start off with , an initial model which is just this global model , and then they split it to the individuals . I don't know yet the what happened Tuesday , but the points that they were supposed to discuss is still , things like the weights ,  professor b: Oh , this is a conference call for , Aurora participant sort of thing . professor b: Do you know who was who was since we weren't in on it , do you know who was in from OGI ? Was was was Hynek involved or was it Sunil phd a: I have no idea . professor b: or ? phd a: Mmm , I just professor b: Oh , you don't know . So the points were the the weights how to weight the different error rates that are obtained from different language and and conditions . phd a: Some people are arguing that it would be better to have weights on  well , to to combine error rates before computing improvement .  , and the fact is that for right now for the English , they have weights they they combine error rates , but for the other languages they combine improvement . And right now actually there is a thing also , that happens with the current weight is that a very non - significant improvement on the well - matched case result in huge differences in in the final number . phd c: How should that be done ?  , it it seems like there 's a simple way phd a:   phd c: Th - they 're professor b: But the but , the other thing phd a: In professor b: I don't know I haven't thought it through , but one one would think that each It it 's like if you say what 's the what 's the best way to do an average , an arithmetic average or a geometric average ? phd c:   professor b: So phd c: Well , it seems like they should do , like , the percentage improvement or something , rather than the absolute improvement . But the question is , do you average the relative improvements or do you average the error rates and take the relative improvement maybe of that ? phd a: Yeah . professor b: And the thing is it 's not just a pure average because there are these weightings . And so when you average the the relative improvement it tends to to give a lot of of , importance to the well - matched case because the baseline is already very good and , i it 's phd c: Why don't they not look at improvements but just look at your av your scores ? You know , figure out how to combine the scores phd a:   phd c: with a weight or whatever , and then give you a score here 's your score . professor b: Well , that 's what he 's seeing as one of the things they could do . professor b: It 's just when you when you get all done , I think that they pro I m I I wasn't there but I think they started off this process with the notion that you should be significantly better than the previous standard . professor b: And , so they said " how much is significantly better ? what do you ? " And and so they said " well , you know , you should have half the errors , " or something , " that you had before " . professor b: i i it does seem like it 's more logical to combine them first and then do the phd a: Combine error rates and then professor b: Yeah . When when you combine error rate it tends to give more importance to the difficult cases , and some people think that professor b: Oh , yeah ? phd a: well , they have different , opinions about this . Some people think that it 's more important to look at to have ten percent imp relative improvement on well - matched case than to have fifty percent on the m mismatched , and other people think that it 's more important to improve a lot on the mismatch and So , bu phd c: It sounds like they don't really have a good idea about what the final application is gonna be . professor b: Well , you know , the the thing is that if you look at the numbers on the on the more difficult cases , if you really believe that was gonna be the predominant use , none of this would be good enough . professor b: whereas you sort of with some reasonable error recovery could imagine in the better cases that these these systems working . So , I think the hope would be that it would  , it would work well for the good cases and , it would have reasonable reas soft degradation as you got to worse and worse conditions . I I guess what I 'm  , I I was thinking about it in terms of , if I were building the final product and I was gonna test to see which front - end I 'd I wanted to use , I would try to weight things depending on the exact environment that I was gonna be using the system in . professor b: I th phd c: So if if they don't know , doesn't that suggest the way for them to go ?  , you assume everything 's equal .  , y y  , you professor b: Well , I I think one thing to do is to just not rely on a single number to maybe have two or three numbers , phd c: Yeah . professor b: and and and say here 's how much you ,  you improve the ,  the the relatively clean case and here 's or or well - matched case , and here 's how here 's how much you , phd c:   professor b: So , I guess what you would do in practice is you 'd try to get as many , examples of similar sort of stuff as you could , and then , phd c: Yeah . professor b:  So the argument for that being the the the more important thing , is that you 're gonna try and do that , but you wanna see how badly it deviates from that when when when the ,  it 's a little different . phd c: So professor b: phd c: so you should weight those other conditions v very you know , really small . That 's a that 's a that 's an arg phd c: that 's more of an information kind of thing . professor b: that 's an ar Well , that 's an argument for it , but let me give you the opposite argument . professor b: are you gonna have w  , examples with the windows open , half open , full open ? Going seventy , sixty , fifty , forty miles an hour ? On what kind of roads ? phd c:   professor b: I I I think that you could make the opposite argument that the well - matched case is a fantasy . professor b: I think the thing is is that if you look at the well - matched case versus the po you know , the the medium and the and the fo and then the mismatched case , we 're seeing really , really big differences in performance . You wouldn't like that as soon as you step outside You know , a lot of the the cases it 's is phd c: Well , that 'll teach them to roll their window up . professor b: in these cases , if you go from the the , I don't remember the numbers right off , but if you if you go from the well - matched case to the medium , it 's not an enormous difference in the in the the training - testing situation , and and and it 's a really big performance drop . professor b: You know , so ,  Yeah ,  the reference one , for instance this is back old on ,  on Italian  , was like six percent error for the well - matched and eighteen for the medium - matched and sixty for the for highly - mismatched .  , and , you know , with these other systems we we helped it out quite a bit , but still there 's there 's something like a factor of two or something between well - matched and medium - matched . And so I think that if what you 're if the goal of this is to come up with robust features , it does mean So you could argue , in fact , that the well - matched is something you shouldn't be looking at at all , that that the goal is to come up with features that will still give you reasonable performance , you know , with again gentle degregra degradation , even though the the testing condition is not the same as the training . professor b: So , you know , I I could argue strongly that something like the medium mismatch , which is you know not compl pathological but  , what was the the medium - mismatch condition again ? phd a: it 's Yeah . Medium mismatch is everything with the far microphone , but trained on , like , low noisy condition , like low speed and or stopped car and tested on high - speed conditions , I think , like on a highway and professor b: Right . phd a: So professor b: So it 's still the same same microphone in both cases , phd a: Same microphone but Yeah . But the way they have it now , it 's I guess it 's it 's They they compute the relative improvement first and then average that with a weighting ? phd a: Yeah . professor b: so , u i since they have these three categories , it seems like the reasonable thing to do is to go across the languages and to come up with an improvement for each of those . professor b: Just say " OK , in the in the highly - matched case this is what happens , in the m the ,  this other m medium if this happens , in the highly - mismatched that happens " . professor b: I think that that I I I gather that in these meetings it 's it 's really tricky to make anything ac make any policy change because everybody has has , their own opinion phd a:   Yeah , but there is probably a a big change that will be made is that the the baseline th they want to have a new baseline , perhaps , which is , MFCC but with a voice activity detector . So they want to have at least fifty percent improvement on the baseline , but w which would be a much better baseline . phd a: And if we look at the result that Sunil sent , just putting the VAD in the baseline improved , like , more than twenty percent , professor b:   phd a: which would mean then then mean that fifty percent on this new baseline is like , well , more than sixty percent improvement on on o e e  professor b: So nobody would be there , probably . professor b: So whose VAD is Is is this a ? phd a: they didn't decide yet . I guess i this was one point of the conference call also , but mmm , so I don't know .  , it 's not that the design of the VAD isn't important , but it 's just that it it it does seem to be i  , a lot of work to do a good job on on that and as well as being a lot of work to do a good job on the feature design , phd a: Yeah . Per - e s s someone told that perhaps it 's not fair to do that because the ,  to make a good VAD you don't have enough to with the the features that are the baseline features . phd c: Wha - what do you mean ? phd a: Yeah , if i professor b: So y so you m s Yeah , but Well , let 's say for ins see , MFCC for instance doesn't have anything in it , related to the pitch . So suppose you 've that what you really wanna do is put a good pitch detector on there and if it gets an unambiguous phd c: Oh , oh . professor b: if it gets an unambiguous result then you 're definitely in a in a in a voice in a , s region with speech . phd c: So there 's this assumption that the v the voice activity detector can only use the MFCC ? phd a: That 's not clear , but this e professor b: Well , for the baseline . professor b: So so if you use other features then y But it 's just a question of what is your baseline . Right ? What is it that you 're supposed to do better than ? phd c: I g Yeah . professor b: And so having the baseline be the MFCC 's means that people could choose to pour their ener their effort into trying to do a really good VAD phd c: I don't s But they seem like two separate issues . phd c: Right ?  professor b: Unfortunately there 's coupling between them , which is part of what I think Stephane is getting to , is that you can choose your features in such a way as to improve the VAD . professor b: You should do both phd c: Right ? professor b: and and I I think that this still makes I still think this makes sense as a baseline . professor b: you know , we had the MFCC 's before , lots of people have done voice activity detectors , phd a:   professor b: you might as well pick some voice activity detector and make that the baseline , just like you picked some version of HTK and made that the baseline .  , and if one of the ways you make it better is by having your features be better features for the VAD then that 's so be it . professor b: But , at least you have a starting point that 's  , cuz i i some of the some of the people didn't have a VAD at all , I guess . professor b: then they they looked pretty bad and and in fact what they were doing wasn't so bad at all . And if it turns out that you can't improve on that , well , then , you know , nobody wins and you just use MFCC .  , it seems like , it should include sort of the current state of the art that you want are trying to improve , and MFCC 's , you know , or PLP or something it seems like reasonable baseline for the features , and anybody doing this task , is gonna have some sort of voice activity detection at some level , in some way . They might use the whole recognizer to do it but rather than a separate thing , but but they 'll have it on some level . phd c: It seems like whatever they choose they shouldn't , you know , purposefully brain - damage a part of the system to make a worse baseline , or professor b: Well , I think people just had phd c: You know ? professor b: it wasn't that they purposely brain - damaged it . professor b: And and then when the the the proposals actually came in and half of them had V A Ds and half of them didn't , and the half that did did well and the half that didn't did poorly . So what happened since , last week is well , from OGI , these experiments on putting VAD on the baseline . And these experiments also are using , some kind of noise compensation , so spectral subtraction , and putting on - line normalization , just after this . So I think spectral subtraction , LDA filtering , and on - line normalization , so which is similar to the pro proposal - one , but with spectral subtraction in addition , and it seems that on - line normalization doesn't help further when you have spectral subtraction . phd c: Is this related to the issue that you brought up a couple of meetings ago with the the musical tones phd a: I phd c: and ? phd a: I have no idea , because the issue I brought up was with a very simple spectral subtraction approach , phd c: Mmm . phd a: and the one that they use at OGI is one from from the proposed the the the Aurora prop  , proposals , which might be much better . And what 's happened here is that we so we have this kind of new , reference system which use a nice a a clean downsampling - upsampling , which use a new filter that 's much shorter and which also cuts the frequency below sixty - four hertz , professor b: Right . professor b: When you say " we have that " , does Sunil have it now , too , phd a: I No .  , it seems to improve on the well - matched case , but it 's a little bit worse on the mismatch and highly - mismatched  when we put the neural network . And with the current weighting I think it 's sh it will be better because the well - matched case is better . professor b: But how much worse since the weighting might change how how much worse is it on the other conditions , when you say it 's a little worse ? phd a: It 's like , fff , fff  , ten percent relative . That 's phd a:  - y w when I say it 's worse , it 's not it 's when I I  , compare proposal - two to proposal - one , so , r  , y putting neural network compared to n not having any neural network . phd a: because it has  , this sixty - four hertz cut - off , clean downsampling , and ,  what else ?  , yeah , a good VAD . I I j  ,  pr professor b: But the latencies but you 've got the latency shorter now . phd f: Isn't it phd a: And so professor b: So it 's better than the system that we had before . phd a: And then I took this system and , mmm , w  , I p we put the old filters also . So we have this good system , with good VAD , with the short filter and with the long filter , and , with the short filter it 's not worse .  professor b: But what you 're saying is that when you do these So let me try to understand . professor b: that , on the i things are somewhat better , in proposal - two for the well - matched case and somewhat worse for the other two cases . professor b: So does ,  when you say ,  So The th now that these other things are in there , is it the case maybe that the additions of proposal - two over proposal - one are less im important ? phd a: Yeah . Then we tried , to do something like proposal - two but having , e using also MSG features . So basically we try to , find good features that could be used for voicing detection , but it 's still ,  on the ,  t phd f: Oh , well , I have the picture . phd a: we w basically we are still playing with Matlab to to look at at what happened , phd c: What sorts of phd f: Yeah . phd a: and phd c: what sorts of features are you looking at ? phd f: We have some phd a: So we would be looking at , the variance of the spectrum of the excitation , phd f: this , this , and this . So the So basically the spectrum of the excitation for a purely periodic sig signal shou sh professor b: OK . Yeah , w what yo what you 're calling the excitation , as I recall , is you 're subtracting the the ,  the mel mel mel filter , spectrum from the FFT spectrum . phd a: So we have the mel f filter bank , we have the FFT , so we just professor b: So it 's it 's not really an excitation , phd a: No . phd f: We have here some histogram , phd a: E yeah , phd f: but they have a lot of overlap . So , well , for unvoiced portion we have something tha that has a mean around O point three , and for voiced portion the mean is O point fifty - nine . phd c: How did you get your voiced and unvoiced truth data ? phd a: We used , TIMIT and we used canonical mappings between the phones phd f: Yeah . We , use TIMIT on this , phd a: and phd f: for phd a: th Yeah . phd f: But if we look at it in one sentence , it apparently it 's good , I think . phd a: It seems quite robust to noise , so when we take we draw its parameters across time for a clean sentence and then nois the same noisy sentence , it 's very close . There could be also the ,  something like the maximum of the auto - correlation function or which phd c: Is this a a s a trained system ? Or is it a system where you just pick some thresholds ? Ho - how does it work ? phd a: Right now we just are trying to find some features . Hopefully , I think what we want to have is to put these features in s some kind of ,  well , to to obtain a statistical model on these features and to or just to use a neural network and hopefully these features w would help phd c: Because it seems like what you said about the mean of the the voiced and the unvoiced that seemed pretty encouraging . phd c: Well , y I I don't know that I would trust that so much because you 're doing these canonical mappings from TIMIT labellings . phd c: Right ? So , really that 's sort of a cartoon picture about what 's voiced and unvoiced . phd c: i it it may be that that you 're finding something good and that the variance is sort of artificial because of how you 're getting your truth . But another way of looking at it might be that  , what w we we are coming up with feature sets after all . So another way of looking at it is that  , the mel cepstru mel spectrum , mel cepstrum , any of these variants , give you the smooth spectrum . By going back to the FFT , you 're getting something that is more like the raw data . So the question is , what characterization and you 're playing around with this another way of looking at it is what characterization of the difference between the raw data and this smooth version is something that you 're missing that could help ? So , looking at different statistical measures of that difference , coming up with some things and just trying them out and seeing if you add them onto the feature vector does that make things better or worse in noise , where you 're really just i i the way I 'm looking at it is not so much you 're trying to f find the best the world 's best voiced - unvoiced , classifier , phd c:   professor b: but it 's more that , you know , try some different statistical characterizations of that difference back to the raw data phd c: Right . The the more obvious is that that well , using the th the FFT , you just it gives you just information about if it 's voiced or not voiced , ma mainly ,  . phd a: this is why we we started to look by having sort of voiced phonemes professor b: Well , that 's the rea w w what I 'm arguing is that 's Yeah . professor b: But in in reality , it 's you know , there 's all of this this overlap and so forth , grad e: Oh , sorry . professor b: and But what I 'm saying is that may be OK , because what you 're really getting is not actually voiced versus unvoiced , both for the fac the reason of the overlap and and then , th you know , structural reasons , like the one that Chuck said , that that in fact , well , the data itself is that you 're working with is not perfect . professor b: So , what I 'm saying is maybe that 's not a killer because you 're just getting some characterization , one that 's driven by your intuition about voiced - unvoiced certainly , phd a:   professor b: but it 's just some characterization of something back in the in the in the almost raw data , rather than the smooth version . professor b: And your intuition is driving you towards particular kinds of , statistical characterizations of , what 's missing from the spectral envelope . professor b: obviously you have something about the excitation , and what is it about the excitation , and , you know and you 're not getting the excitation anyway , you know . So so I I would almost take a  , especially if if these trainings and so forth are faster , I would almost just take a  , a scattershot at a few different ways of look of characterizing that difference and , you could have one of them but and and see , you know , which of them helps . phd c: So i is the idea that you 're going to take whatever features you develop and and just add them onto the future vector ? Or , what 's the use of the the voiced - unvoiced detector ? phd a: I guess we don't know exactly yet . Th phd c: It 's not part of a VAD system that you 're doing ? phd f: No . phd a:  Yeah , it could be ,  it could be a neural network that does voiced and unvoiced detection , phd c:   professor b: But each one of the mixture components  , you have , variance only , so it 's kind of like you 're just multiplying together these , probabilities from the individual features within each mixture . So it 's so , it seems l you know phd c: I think it 's a neat thing .  , I know that , people doing some robustness things a ways back were were just doing just being gross and just throwing in the FFT and actually it wasn't wasn't wasn't so bad .  , so it would s and and you know that i it 's gotta hurt you a little bit to not have a a spectral ,  a s a smooth spectral envelope , so there must be something else that you get in return for that phd a:   phd c: So how does  , maybe I 'm going in too much detail , but how exactly do you make the difference between the FFT and the smoothed spectral envelope ? Wha - wh i i  , how is that ,  ? phd a: we just How did we do it up again ? phd f: we distend the we have the twenty - three coefficient af after the mel f filter , phd a:   phd f: And i the interpolation i between the point is give for the triang triangular filter , the value of the triangular filter and of this way we obtained this mode this model speech . phd a: S professor b: So you essentially take the values that th that you get from the triangular filter and extend them to sor sort of like a rectangle , that 's at that m value . phd a: So we have we have one point for one energy for each filter bank , phd f: mmm Yeah , it 's linear . phd a: which is the energy that 's centered on on on the triangle phd f: Yeah . At the n at the center of the filter phd c: So you you end up with a vector that 's the same length as the FFT vector ? phd a: Yeah . And I think the variance is computed only from , like , two hundred hertz to one to fifteen hundred . phd a: Above ,  it seems that Well , some voiced sound can have also , like , a noisy part on high frequencies , and But professor b: Yeah . phd a: Well , it 's just professor b: No , it 's makes sense to look at low frequencies . phd c: So this is  , basically this is comparing an original version of the signal to a smoothed version of the same signal ? phd f: Yeah . So i so i i this is  , i you could argue about whether it should be linear interpolation or or or or zeroeth order , but but phd c:  - huh . professor b: at any rate something like this is what you 're feeding your recognizer , typically .  , so the mel cepstrum is the is the is the cepstrum of this this , spectrum or log spectrum , phd a: So this is Yeah . professor b: whatever it You - you 're subtracting in in in power domain or log domain ? phd a: In log domain . But , anyway ,  and that 's phd c: So what 's th  , what 's the intuition behind this kind of a thing ? I I don't know really know the signal - processing well enough to understand what what is that doing . What happen if what we have have what we would like to have is some spectrum of the excitation signal , professor b: Yeah . phd a: And the way to do this is that well , we have the we have the FFT because it 's computed in in the in the system , and we have the mel filter banks , phd c:   phd a: and so if we if we , like , remove the mel filter bank from the FFT , we have something that 's close to the excitation signal . phd a: It 's something that 's like a a a train of p a pulse train for voiced sound phd c: OK . So do you have a picture that sh ? phd a: So - It 's Y phd c: Is this for a voiced segment , phd a: yeah . phd c: this picture ? What does it look like for unvoiced ? phd f: Yeah . This is phd f: but between the frequency that we are considered for the excitation phd a: Right . phd c: So , does does the periodicity of this signal say something about the the phd f: Fifteen p phd a: So it 's Yeah . professor b: to first order what you 'd what you 're doing  , ignore all the details and all the ways which is that these are complete lies .  , the the you know , what you 're doing in feature extraction for speech recognition is you have , in your head a a a a simplified production model for speech , phd c:   professor b: in which you have a periodic or aperiodic source that 's driving some filters . phd a: Do you have the mean do you have the mean for the auto - correlation ? professor b: first order for speech recognition , you say " I don't care about the source " . professor b: The filters roughly act like a ,  a ,  a an overall resonant you know , f some resonances and so forth that th that 's processing excitation . professor b: So if you look at the spectral envelope , just the very smooth properties of it , you get something closer to that . professor b: And the notion is if you have the full spectrum , with all the little nitty - gritty details , that that has the effect of both , phd c: Yeah . professor b: And so this is saying , well , if you really do have that sort of vocal tract envelope , and you subtract that off , what you get is the excitation . And I call that lies because you don't really have that , you just have some kind of signal - processing trickery to get something that 's kind of smooth . professor b: That 's why I was going to the why I was referring to it in a more a more , conservative way , when I was saying " well , it 's yeah , it 's the excitation " . This moved in the professor b: So so , stand standing back from that , you sort of say there 's this very detailed representation . professor b: but whenever you smooth you lose something , so the question is have you lost something you can you use ? phd c: Right . professor b: probably you wouldn't want to go to the extreme of just ta saying " OK , our feature set will be the FFT " , cuz we really think we do gain something in robustness from going to something smoother , but maybe there 's something that we missed . professor b: And then you go back to the intuition that , well , you don't really get the excitation , but you get something related to it . professor b: And it and as you can see from those pictures , you do get something that shows some periodicity , in frequency , phd c:   professor b: so , phd c: So you don't have one for unvoiced picture ? phd f: not here . professor b: But presumably you 'll see something that won't have this kind of , regularity in frequency , in the phd a: But Yeah . phd c: And so you said this is pretty doing this kind of thing is pretty robust to noise ? phd a: It seems , yeah . The mean is different with it , because the the histogram for the the classifica phd a: No , no , no . But th the kind of robustness to noise phd f: Oh ! phd a: So if if you take this frame , from the noisy utterance and the same frame from the clean utterance phd f:  . Cool ! phd f: I have here the same frame for the clean speech phd c: Oh , that 's clean . phd f: the same cle phd c: Oh , OK phd f: But they are a difference . phd a: Yeah , that 's phd f: Because here the FFT is only with two hundred fifty - six point phd c: Oh . phd a: because if we use the standard , frame length of of , like , twenty - five milliseconds , what happens is that for low - pitched voiced , because of the frame length , y you don't really have you don't clearly see this periodic structure , professor b:   phd a: Yeah , but it 's the same frame and phd c: Oh , it 's that time - frequency trade - off thing . Well ,  it looks better , but , the thing is if if ,  if you 're actually asking you know , if you actually j  , need to do place along an FFT , it may be it may be pushing things . professor b: And and ,  phd c: Would you would you wanna do this kind of , difference thing after you do spectral subtraction ? phd a: maybe . The spectral subtraction is being done at what level ? Is it being done at the level of FFT bins or at the level of , mel spectrum or something ? phd a: I guess it depends . professor b: how are they doing it ? phd a: How they 're doing it ? Yeah . Filter bank , phd a: no ? It 's on the filter bank , phd f: yeah . So we 'll perhaps try to convince OGI people to use the new the new filters and Yeah .  , has has anything happened yet on this business of having some sort of standard , source , phd a: not yet professor b: or ? phd a: but I wi I will call them and professor b: OK . phd a: now they are I think they have more time because they have this well , Eurospeech deadline is over phd c: When is the next , Aurora deadline ? phd a: and It 's , in June . professor b: Early June , late June , middle June ? phd a: I don't know w professor b:  .  , and he 's been doing all the talking but but these he 's he 's ,  phd f: Yeah .  , but has he pretty much been talking about what you 're doing also , and ? phd f: Oh , I I am doing this . I 'm sorry , but I think that for the recognizer for the meeting recorder that it 's better that I don't speak . phd f: Because professor b: You know , we 'll get we 'll get to , Spanish voices sometime , and we do we want to recognize , you too . phd f: After the after , the result for the TI - digits on the meeting record there will be foreigns people . phd c: Y professor b: We like we we 're we 're w we are we 're in the , Bourlard - Hermansky - Morgan , frame of mind . So it 's  , anything to talk about ? grad d: N  , not not not much is new . So when I talked about what I 'm planning to do last time , I said I was , going to use Avendano 's method of , using a transformation , to map from long analysis frames which are used for removing reverberation to short analysis frames for feature calculation .  , but , I decided not to do that after all because I I realized to use it I 'd need to have these short analysis frames get plugged directly into the feature computation somehow professor b:   grad d: and right now I think our feature computation is set to up to , take , audio as input , in general . So I decided that I I 'll do the reverberation removal on the long analysis windows and then just re - synthesize audio and then send that . Right ? grad d: or or even if I 'm using our system , I was thinking it might be easier to just re - synthesize the audio , professor b: Yeah ? grad d: because then I could just feacalc as is and I wouldn't have to change the code .  , longer - term if it 's if it turns out to be useful , one one might want to do something else , grad d: Right . professor b: but  , in in other words , you you may be putting other kinds of errors in from the re - synthesis process . But anyway it sounds like a reasonable way to go for a for an initial thing , and we can look at at exactly what you end up doing and and then figure out if there 's some something that could be be hurt by the end part of the process . So that 's That was it , huh ? grad d: That Yeah , e That 's it , that 's it . I went off on a little tangent this past week , looking at , modulation s spectrum stuff , and and learning a bit about what what ,  what it is , and , the importance of it in speech recognition . And I found some some , neat papers , historical papers from , Kanedera , Hermansky , and Arai . grad e: And they they did a lot of experiments where th where , they take speech and , e they modify the ,  they they they measure the relative importance of having different , portions of the modulation spectrum intact . grad e: And they find that the the spectrum between one and sixteen hertz in the modulation is ,  is im important for speech recognition . professor b: And and , the the MSG features were sort of built up with this notion grad e: Yeah . professor b: But , I guess , I thought you had brought this up in the context of , targets somehow . professor b: But i m grad e:  professor b: i it 's not  , they 're sort of not in the same kind of category as , say , a phonetic target or a syllabic target grad e: Mmm . professor b: or a grad e: I was thinking more like using them as as the inputs to to the detectors . professor b: So maybe , le phd c: Should we do digits ? professor b: let 's do digits 