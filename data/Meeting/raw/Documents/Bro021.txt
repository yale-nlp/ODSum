I 'm sick of being the one to sort of go through and say , " Well , what do you think about this ? " You wanna ? phd d: Yeah . phd f: Should we take turns ? You want me to run it today ? professor b: Yeah . Let 's see , maybe we should just get a list of items things that we should talk about .  , I guess there 's the usual updates , everybody going around and saying , you know , what they 're working on , the things that happened the last week . phd f: This is what what do you ,  what 's in the paper there ? phd c: So it 's the paper that describe basically the , system that were proposed for the Aurora . phd f: The one that we s we submitted the last round ? phd c: Right , yeah . phd f: Where where 's it gonna be this year ? phd c: It 's , Aalborg in Denmark . Then , whhh well , I 've been working on on t mainly on on - line normalization this week .  , second thing is , the training of , on - line normalization with two different means , one mean for the silence and one for the speech .  , and so I have two recursions which are controlled by the , probability of the voice activity detector . professor b: So do you maybe make errors in different places ? Different kinds of errors ? phd c: I didn't look , more closely . Well , eh , there is one thing that we can observe , is that the mean are more different for for C - zero and C - one than for the other coefficients . And Yeah , it the C - one is There are strange strange thing happening with C - one , is that when you have different kind of noises , the mean for the the silence portion is can be different . phd c: So when you look at the trajectory of C - one , it 's has a strange shape and I was expecting th the s that these two mean helps , especially because of the the strange C - ze C - one shape , which can like , yo you can have , a trajectory for the speech and then when you are in the silence it goes somewhere , but if the noise is different it goes somewhere else . phd c: So which would mean that if we estimate the mean based on all the signal , even though we have frame dropping , but we don't frame ev  , drop everything , but  , this can hurts the estimation of the mean for speech , and Mmm .  , a third thing is , that instead of t having a fixed time constant , I try to have a time constant that 's smaller at the beginning of the utterances to adapt more quickly to the r something that 's closer to the right mean . phd c: well , if it 's higher than a certain threshold , I keep it to this threshold to still , adapt , the mean when if the utterance is , long enough to to continue to adapt after , like , one second professor b:   It seems pretty phd f: Wasn't there some experiment you were gonna try where you did something differently for each ,  I don't know whether it was each mel band or each , FFT bin or someth There was something you were gonna  , some parameter you were gonna vary depending on the frequency . I don't know if that was phd c: I guess it was I don't know . u Maybe it 's this this idea of having different on - line normalization , tunings for the different MFCC 's . And then it was something about , some and then somebody said " yeah , it does seem like , you know , C - zero is the one that 's , you know , the major one " or , s I can't remember exactly what it was now . S  , it 's very important to normalize C - zero and much less to normalize the other coefficients . And we I think , we kind of know that normalizing C - one doesn't help with the current scheme . In my idea , I I was thinking that the the the reason is maybe because of these funny things that happen between speech and silence which have different means . But maybe it 's not so so easy to professor b: I I really would like to suggest looking , a little bit at the kinds of errors . I know you can get lost in that and go forever and not see too much , but sometimes , phd c:   professor b: but but , just seeing that each of these things didn't make things better may not be enough . It may be that they 're making them better in some ways and worse in others , phd c: Yeah . professor b: or increasing insertions and decreasing deletions , or or , you know , helping with noisy case but hurting in quiet case . And if you saw that then maybe you it would something would occur to you of how to deal with that . I 've been playing a little bit with some kind of thresholding , and , mmm , as a first experiment , I think I Yeah . Well , what I did is t is to take ,  to measure the average no , the maximum energy of s each utterance and then put a threshold Well , this for each mel band . Then put a threshold that 's fifteen DB below well , a couple of DB below this maximum , professor b:   phd c: So I was adding a white noise energy , that 's fifteen DB below the maximum energy of the utterance . When we look at at the , MFCC that result from this , they are a lot more smoother .  , when we compare , like , a channel zero and channel one utterance  , so a clean and , the same noisy utterance well , there is almost no difference between the cepstral coefficients of the two . And the result that we have in term of speech recognition , actually it 's not it 's not worse , it 's not better neither , but it 's , kind of surprising that it 's not worse phd f:  . phd c: because basically you add noise that 's fifteen DB just fifteen DB below the maximum energy . phd c: And at least phd f: So why does that m smooth things out ? I don't I don't understand that . Right ? phd c: It 's I think , it 's whitening This the portion that are more silent , professor b: Cuz it 's phd c: as you add a white noise that are has a very high energy , it whitens everything phd f: Huh . phd c: and and the high - energy portion of the speech don't get much affected anyway by the other noise . And and professor b: So , again , if you trained in one kind of noise and tested in the same kind of noise , you 'd you know , given enough training data you don't do b do badly . The reason that we d that we have the problems we have is because it 's different in training and test . professor b: so when you whiten it , then it 's like you the the only noise to to first order , the only th noise that you have is white noise and you 've added the same thing to training and test . professor b:  phd f: So would that be similar to , like , doing the smoothing , then , over time or ? phd c:   professor b: Well , it 's a kind of smoothing , phd c: I think it 's I think it 's different . professor b: but phd c: It 's it 's something that yeah , that affects more or less the silence portions because phd f:   phd c: Well , anyway , the sp the portion of speech that ha have high energy are not ch a lot affected by the noises in the Aurora database . phd c: If if you compare th the two shut channels of SpeechDat - Car during speech portion , it 's n n n the MFCC are not very different . And , professor b: Yeah , but you 're still getting more recognition errors , phd c:  professor b: which means that the differences , even though they look like they 're not so big , are are hurting your recognition . So , but in this case I I really expect that maybe the the two these two stream of features , they are very different .  , and maybe we could gain something by combining them professor b: Well , the other thing is that you just picked one particular way of doing it . phd c: or professor b: first place it 's fifteen DB , down across the utterance . phd f: So what was the what was the threshold part of it ? Was the threshold , how far down ? professor b: Yeah . How does it go ? If it if if the peak value 's above some threshold , then you add the noise ? Or if it 's below s phd c: I systematically add the noise , but the , noise level is just some kind of threshold below the peak . professor b: So then afterwards a log is taken , and that 's so sort of why the the little variation tends to go away . And I don't know , maybe a constant noise addition would would be fine also , or  professor b: Or or not constant but but , varying over time in fact is another way to go .  professor b: Were you using the the normalization in addition to this ?  , what was the rest of the system ? phd c:  Yeah . A third thing is that , I play a little bit with the ,  finding what was different between , And there were a couple of differences , like the LDA filters were not the same .  , so when we put s some noise compensation the , LDA filter that that 's derived from noisy speech is not more anymore optimal .  , if we use the the old LDA filter ,  the LDA filter that was in the proposal , we have , like , eighty - two point seven percent recognition rate , on noisy speech when the system is trained on clean speech . But and when we use the filter that 's derived from clean speech we jumped so from eighty - two point seven to eighty - five point one , which is a huge leap . So now the results are more similar , and I don't I will not , I think , investigate on the other differences , which is like the number of MFCC that we keep and other small things that we can I think optimize later on anyway . But on the other hand if everybody is trying different kinds of noise suppression things and so forth , it might be good to standardize on the piece that we 're not changing . Right ? So if there 's any particular reason to ha pick one or the other ,  Which which one is closer to what the proposal was that was submitted to Aurora ? Are they they both ? Well ,  phd c: I think Yeah . I think th th  , the new system that I tested is , I guess , closer because it doesn't have it have less of of France Telecom stuff , phd d: You mean the phd c: I phd d: The whatever you , tested with recently . phd d: Yeah ? professor b: Well , no , I I 'm I Yeah , you 're trying to add in France Telecom . Right ? Or phd d: The number of cepstral coefficients is what ? professor b: Cep phd c:   So , I think we 'd wanna standardize there , wouldn't we ? phd c: Yeah , yeah . phd c: I think we were gonna work with with this or this new system , or with phd d: so the the right now , the the system that is there in the what we have in the repositories , with uses fifteen . So , we haven't w we have been always using , fifteen coefficients , phd c: Yeah . Then professor b: I think as long as you guys agree on it , it doesn't matter . phd d: mmm professor b: I think we have a maximum of sixty , features that we 're allowed . Ma - maybe we can  , at least , I 'll t s run some experiments to see whether once I have this noise compensation to see whether thirteen and fifteen really matters or not . phd d: Never tested it with the compensation , but without , compensation it was like fifteen was s slightly better than thirteen , phd c: Yeah . phd d: You know , always for the matched condition , you always get a slightly better performance for log energy than C - zero . phd d: But not for  , for matched and the clean condition both , you get log energy  you get a better performance with log energy . phd d: Well , maybe once we have this noise compensation , I don't know , we have to try that also , whether we want to go for C - zero or log energy . phd f: So do you have more , Stephane , or ? phd c: that 's it , I think . phd f: How about you , Barry ? grad a: still working on my my quals preparation stuff .  , so I 'm I 'm thinking about , starting some , cheating experiments to , determine the ,  the relative effectiveness of , some intermediate categories that I want to classify . So , for example , if I know where voicing occurs and everything , I would do a phone  , phone recognition experiment , somehow putting in the the ,  the perfect knowledge that I have about voicing . So , in particular I was thinking , in in the hybrid framework , just taking those LNA files , and , setting to zero those probabilities that ,  that these phones are not voicing . So say , like , I know this particular segment is voicing , I would say , go into the corresponding LNA file and zonk out the the posteriors for , those phonemes that , are not voiced , phd f:   And so this would be a useful thing , to know in terms of , like , which which ,  which of these categories are are good for , speech recognition . grad a: So , that 's I hope to get those ,  those experiments done by by the time quals come come around in July . phd f: So do you just take the probabilities of the other ones and spread them out evenly among the the remaining ones ? grad a: Yeah . I I I was thinking OK , so just set to set to some really low number , the the non - voiced , phones . phd f: wh are you gonna do digits grad a: Yeah , m  , well , I 'm gonna f work with TIMIT phd f: or ? With TIMIT . So where do the outputs of the net go into if you 're doing phone recognition ? grad a: Oh . So maybe , Chronos phd f: An - and you 're gonna the you 're gonna do phone recognition with that ? grad a: or Phone recognition . And , another thing would be to extend this to , digits or something where I can look at whole words . grad a: And I would be able to see , not just , like , phoneme events , but , inter - phoneme events . So , like , this is from a stop to to a vo a vocalic phd f:   phd f:  Let 's see , I haven't done a whole lot on anything related to this this week . Well , in my lunch talk last week I I said I 'd tried phase normalization and gotten garbage results using that l  , long - term mean subtraction approach . And also I 've been talking to , Andreas and Thilo about the , SmartKom language model and about coming up with a good model for , far mike use of the SmartKom system . So I 'm gonna be working on , implementing this mean subtraction approach in the far - mike system for the SmartKom system ,  . And , one of the experiments we 're gonna do is , we 're gonna , train the a Broadcast News net , which is because that 's what we 've been using so far , and , adapt it on some other data .  , An - Andreas wants to use , data that resembles read speech , like these digit readings , because he feels that the SmartKom system interaction is not gonna be exactly conversational . grad g: S so actually I was wondering , how long does it take to train that Broadcast News net ? professor b: The big one takes a while . professor b: So but , you know , you can get I don't know if you even want to run the big one , in the in the final system , cuz , you know , it takes a little while to run it . So , you can scale it down by I 'm sorry , it was two , three weeks for training up for the large Broadcast News test set training set . professor b: i so if you trained on half as much and made the net , half as big , then it would be one fourth the amount of time grad g: OK . Also , I guess we had we 've had these , little di discussions I guess you ha haven't had a chance to work with it too much about about ,  m other ways of taking care of the phase . So , I I guess that was something I could say would be that we 've talked a little bit about grad g:   professor b: you just doing it all with complex arithmetic and ,  and not not , doing the polar representation with magnitude and phase . But it looks like there 's ways that one could potentially just work with the complex numbers and and and in principle get rid of the effects of the average complex spectrum . But grad g: And , actually , regarding the phase normalization So I did two experiments , and one is So , phases get added , modulo two pi , and because you only know the phase of the complex number t t to a value modulo two pi . And so I thought at first , that , what I should do is unwrap the phase because that will undo that .  , but I actually got worse results doing that unwrapping using the simple phase unwrapper that 's in Matlab than I did not unwrapping at all . So I 'm I 'm still hopeful that that  , we we don't even know if the phase is something the average phase is something that we do want to remove . But , at least in principle it looks like there 's there 's , a couple potential ways to do it . So you work with the complex numbers and then when you get the spectrum the average complex spectrum  , actually divide it out , as opposed to taking the log and subtracting . And , actually I was talking to Dick Karp about it a little bit , and and and , since I got thinking about it , and and , so one thing is that y you 'd have to do , I think ,  we may have to do this on a whiteboard , but I think you have to be a little careful about scaling the numbers that you 're taking the complex numbers that you 're taking the log of because the Taylor expansion for it has , you know , a square and a cube , and and so forth . And and so if if you have a a number that is modulus , you know , very different from one It should be right around one , if it 's cuz it 's a expansion of log one one minus epsilon or o is is one plus epsilon , or is it one plus ? Well , there 's an epsilon squared over two and an epsilon cubed over three , grad g: OK . But that 's not a big deal cuz it 's the log of of K times a complex number , then you can just that 's the same as log of K plus log of the complex number . How about you , Sunil ? phd d: So , I 've been , implementing this , Wiener filtering for this Aurora task . And then I ran the whole recognition experiment with Italian and I got , like , worse results than not using it . And then it looks like I have some problem in the way there is some some very silly bug somewhere . I was looking at the spectrograms that I got and it 's , like w it 's it 's very horrible . Like , when I professor b: I I missed the v I 'm sorry , I was I was distracted . phd d: And it it it gave , like I just got the signal out and it it was OK . So , I plugged it in somewhere and then  , it 's like I had to remove some part and then plugging it in somewhere . So , it was real  , I thought it was all fine and then I ran it , and I got something worse than not using it . So , I was like I 'm trying to find where the m m problem came , professor b:  - huh . And , the other thing , was ,  Well , Hynek showed up one suddenly on one day and then I was t talking wi professor b: Right . So I was actually that day I was thinking about d doing something about the Wiener filtering , and then Carlos matter of stuff . And then he gave me a whole bunch of filters what Carlos used for his , thesis and then that was something which came up . And then ,  So , I 'm actually , thinking of using that also in this , W Wiener filtering because that is a m modified Wiener filtering approach , where instead of using the current frame , it uses adjacent frames also in designing the Wiener filter . So instead of designing our own new Wiener filters , I may just use one of those Carlos filters in in this implementation professor b:   phd d: and see whether it it actually gives me something better than using just the current f current frame , which is in a way , something like the smoothing the Wiener filter professor b:   phd d: but @ @ S so , I don't know , I was h I 'm I 'm I 'm , like that so that is the next thing . Once this I once I sort this pro  , problem out maybe I 'll just go into that also . So , I , like , plugged some groupings for computing this eigen  , s values and eigenvectors . So just I just @ @ some small block of things which I needed to put together for the subspace approach . phd f: What what is VTS again ? phd d: New phd e: Eh , Vectorial Taylor Series . I think I ask you that every single meeting , don't I ? phd e: What ? phd f: I ask you that question every meeting . phd e: If Well professor b: It 's good to have some , cases of the same utterance at different different times . And we put everything , the result is better , but it 's not better than the result that we have without VTS . So that @ @ given that you 're using the VAD also , the effect of the VTS is not so far phd e: Is not . professor b: Do you How much of that do you think is due to just the particular implementation and how much you 're adjusting it ? Or how much do you think is intrinsic to ? phd e: Pfft . I don't know because phd c: Are you still using only the ten first frame for noise estimation phd e: Hhh , phd c: or ? Or i ? phd e: I do the experiment using only the f onl eh , to use on only one fair estimation of the noise . And , well , it 's a little bit better but not n phd c: Maybe you have to standardize this thing also , noise estimation , because all the thing that you are testing use a different They all need some some noise noise spectra phd d: Mmm .  , given that we 're going to have for this test at least of  , boundaries , what if initially we start off by using known sections of nonspeech for the estimation ? phd c:   professor b: first place ,  even if ultimately we wouldn't be given the boundaries , this would be a good initial experiment to separate out the effects of things .  , how much is the poor you know , relatively , unhelpful result that you 're getting in this or this or this is due to some inherent limitation to the method for these tasks and how much of it is just due to the fact that you 're not accurately finding enough regions that that are really n noise ? phd d: Mmm . So maybe if you tested it using that , you 'd have more reliable stretches of nonspeech to do the estimation from and see if that helps . Well , we have If this if this is the noise signal , in the log domain , we have something like this . And the idea of these methods is to n given a ,  professor b:  phd e: How do you say ? I will read because it 's better for my English . I i given is the estimate of the PDF of the noise signal when we have a , a statistic of the clean speech and an statistic of the noisy speech . So , the top equation is is is phd e: No , this in the it 's this is the log domain . professor b: Which is which is the log domain ? phd e: Is the T is egual is equal to , log of professor b: And but Y is what ? Y of the spectrum phd e: this this is this professor b: or ? phd e: and this is this . phd c: p s this professor b: No , is that power spectrum ? Is it ? phd c: Yeah . And professor b: So that 's  phd e: This is the noisy Yeah , it 's professor b: OK . professor b: OK , so you have power spectrum added there and down here you have you you put the depends on T , but b all of this is just you just mean phd e: w o Yeah . phd e: Yeah , maybe professor b: o phd e: But , n Well , y we can expre we can put this expression professor b: X times one plus , N  , N N N minus X ? phd e: The Yeah . professor b: And then ,  So that 's log of X plus log of one plus ,  phd e: And the noise signal . phd e: Well , if we apply the log , we have E is n phd c: Mmm . phd d: and phd e: log E is equal , oh , to log of X plus N . phd e: And , well , phd d: And , log of phd e: we can say that E is equal to log of , exponential of X plus exponential of N . phd d: Well , if E restricts It is y phd e: Well , this is this is in the ti the time domain . phd e: and we can put u that n the log domain log of X omega , but , well , in the time domain we have an exponential .  , it 's just if X and N are variables Right ? phd d: What is ,  ? professor b: The the the log of X plus N is not the same as the log of E to the X plus E to the N . But this i Well , I don't Well , professor b: Maybe we can take it off - line , phd e: maybe professor b: but I I don't know . Well , the expression that appear in the in the paper , is ,  phd d: The log the Taylor series expansion for log one plus N by X is professor b: OK . phd c: Is it the first - order expansion ? phd e: is X professor b: I i phd d: Yeah , the first one . If if you take log X into log one plus N by X , and then expand the log one plus N by X into Taylor series phd c: Yeah . phd e: Now , this is the and then phd c: Yeah , but the the second expression that you put is the first - order expansion of the nonlinear relation between phd e: Not exactly . Well , we have pfft , em Well , we can put that X is equal I is equal to log of , mmm professor b: That doesn't follow . Because cuz the log of a sum is not the same as th phd e: The top ? professor b: as phd e: Yeah , yeah , yeah , yeah , yeah . phd e: But we can  , we we know that , for example , the log of E plus B is equal to log of E plus log to B . So you could s phd c: What is that ? phd e: And we can , put this inside . phd e: And then we can , professor b: N no , phd e: you know professor b: but phd e: Yeah . professor b: The  , just more generally here , if you say " log of , A plus B " , the log of log of A plus B is not or A plus B is not the , log of E to the A plus E to the B . professor b: Right ? Cuz you cuz you up here you have the A plus B phd e: No . I say if I apply log , I have , log of E is equal to log of ,  in this side , is equal to log of X professor b: Plus N . professor b: And then how do you go from there to the ? phd e: This is right . OK , so let 's  , C equals A plus B , phd c: It 's log o of capital Y . It 's just by definition that the individual that the ,  So , capital X is by definition the same as E to the little X because she 's saying that the little X is is the ,  is the log . professor b: I think these things are a lot clearer when you can use fonts different fonts there phd e: Oh , yes . phd e: And now I can do it ,  pfff ! I can put log of EX plus log professor b: Oh . So now once you get that that one , then you then you do a first or second - order , or something , Taylor series expansion of this . And for that , well , the goal is to obtain ,  est estimate a PDF for the noisy speech when we have a a statistic for clean speech and for the noisy speech . Mmm ? And when w the way to obtain the PDF for the noisy speech is well , we know this statistic and we know the noisy st well , we can apply first order of the vector st Taylor series of the of the of well , the order that we want , increase the complexity of the problem . phd e: And then when we have a expression , for the mean and variance of the noisy speech , we apply a technique of minimum mean - square estimation professor b:   phd e: to obtain the expected value of the clean speech given the this statistic for the noisy speech professor b:   phd e: We can expre we can put that the PDF for the clean test , probability of the clean speech is equal to professor b: Yeah . professor b: So , how h how much in in the work they reported , how much noisy speech did you need to get , good enough statistics for the to get this mapping ? phd e: I don't know exactly . professor b: Cuz I think what 's certainly characteristic of a lot of the data in this test is that , you don't have the the training set may not be a a great estimator for the noise in the test set . And what are you using for the noisy ? Y y doing that strictly phd e: Of the noise I estimate the noises wi professor b:   professor b: And and you and you train it up entirely from , nonspeech sections in the test ? phd c:  . The first experiment that I do it is solely to calculate the , mmm well , this value professor b: Yeah . phd e: the compensation of the dictionary o one time using the the noise at the f beginning of the sentence .  , because well , the VTS methods In fact the first thing that I do is to to obtain , an expression for E probability e expression of of E . That mean that the VTS mmm , with the VTS we obtain ,  well , we we obtain the means for each Gaussian and the variance . phd e: Because we can write  , we can write that the estimation of the clean speech is equal at an expected value of the clean speech conditional to , the noise signal the probability f of the the statistic of the clean speech and the statistic of the noise . phd e: And we can put that this is equal to the estimated value of E minus a function that conditional to E to the T to the noise signal . Well , this is this function is the the term after develop this , the term that we we take . phd e: And I can put that this is equal to the noise signal minus Well , I put before this name ,  And I can calculate this . professor b: What is the first variable in that probability ? phd e: this is the Gaussian .  , if we have clean speech we have the dictionary for the clean speech , we have a probability f of our our weight for each Gaussian . phd e: And for calculate this , I have an I I can develop an expression that is phd d: It 's overlapping . I can calculate I can I calculated this value , with the statistic of the noisy speech that I calculated before with the VTS approximation .  , with the , nnn when I develop this in s Taylor Taylor series , I can't , calculate the mean and the variance of the for each of the Gaussian of the dictionary for the noisy speech . phd e: If I never do an estimat a newer estimation of the noise , this mean as mean and the variance are fixed . phd e: And for each s  , frame of the speech the only thing that I need to do is to calculate this in order to calculate the estimation of the clean speech given our noisy speech . professor b: So , I 'm I 'm not following this perfectly but , I Are you saying that all of these estimates are done using , estimates of the probability density for the noise that are calculated only from the first ten frames ? And never change throughout anything else ? phd e: Yeah . phd e: And the other estimation is when I do the  on - line estimation , I change the means and variance of th for the noisy speech professor b: Yeah ? phd e: each time that I detect noise . professor b: So you estimated , f completely forgetting what you had before ?  , or is there some adaptation ? phd e: no , no , no . It 's not completely No , it 's I am doing something like an adaptation of the noise . Now do we know , either from their experience or from yours , that , just having , two parameters , the the mean and variance , is enough ? Yeah .  , I know you don't have a lot of data to estimate with , but but ,  phd e: I estimate mean and variance for each one of the Gaussian of the codebook . phd e: I don't know i professor b: And you and and it 's ,  right , it 's only it 's only one Wait a minute . This is what 's the dimensionality of the Gaussian ? This is phd e: it 's in after the mel filter bank . Yeah , maybe maybe you don't have a phd e: the original paper say that only one Gaussian for the noise . But , no no paper is is a Bible , phd e: Yeah , maybe isn't the right thing . professor b: The question is , whether it would be helpful , i particularly if you used if you had more So , suppose you did This is almost cheating . But if y suppose you use the real boundaries that that you were in fact were given by the VAD and so forth or I I guess we 're gonna be given even better boundaries than that . And you look you take all o all of the nonspeech components in an utterance , so you have a fair amount . professor b: So first question would be to what extent i are the errors that you 're still seeing based on the fact that you have poor boundaries for the , nonspeech ? And the second question might be , given that you have good boundaries , could you do better if you used more parameters to characterize the noise ?  . Also another question might be  , they are doing they 're using first term only of the vector Taylor series ? phd e: Yeah . professor b: if you do a second term does it get too complicated cuz of the nonlinearity ? phd e: Yeah . phd e: Oh , it 's it 's the for me it 's the first time that I am working with VTS . phd e:  professor b: w we haven't had anybody work with it before , so it 's interesting to get your get your feedback about it . phd e: It 's another type of approximation because i because it 's a statistic statistic approximation to remove the noise . phd f: professor b: They prefer to have them on phd f: and the professor b: just so that they 're continuing to get the distant , information 