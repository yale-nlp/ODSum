professor b: So what what from what grad a: Hello ? professor b: Whatever we say from now on , it can be held against us , right ? phd e: That 's right . So I I the the problem is that I actually don't know how th these held meetings are held , if they are very informal and sort of just people are say what 's going on phd e: Yeah . phd e: We just sorta go around and people say what 's going on , what 's the latest  professor b: Yeah . So I guess that what may be a reasonable is if I  first make a report on what 's happening in Aurora in general , at least what from my perspective . professor b: And and  so , I I think that Carmen and Stephane reported on  Amsterdam meeting , phd d:  o professor b: which was kind of interesting because it was for the first time we realized we are not friends really , but we are competitors . phd e: right ? that they were trying to decide ? professor b: There is a plenty of there 're plenty of issues . phd e: Like the voice activity detector , professor b: Well and what happened was that they realized that if two leading proposals , which was French Telecom Alcatel , and us both had  voice activity detector . And I said " well big surprise ,  we could have told you that n n n four months ago , except we didn't because nobody else was bringing it up " . professor b: Obviously French Telecom didn't volunteer this information either , cuz we were working on mainly on voice activity detector for past  several months phd e: Right . professor b: I said " well yeah , you are absolutely right ,  if I wish that you provided better end point at speech because  or at least that if we could modify the recognizer ,  to account for these long silences , because otherwise  that that th that wasn't a correct thing . " And so then ev ev everybody else says " well we should we need to do a new eval evaluation without voice activity detector , or we have to do something about it " . Because  but in that case ,  we would like to change the  the algorithm because  if we are working on different data , we probably will use a different set of tricks . professor b: But unfortunately nobody ever officially can somehow acknowledge that this can be done , because French Telecom was saying " no , no , no , now everybody has access to our code , so everybody is going to copy what we did . " Yeah well our argument was everybody ha has access to our code , and everybody always had access to our code . We thought that people are honest , that if you copy something and if it is protected protected by patent then you negotiate , or something , phd e: Yeah . professor b: But And French Telecom was saying " no , no , no , phd e:   professor b: there is a lot of little tricks which  sort of  cannot be protected and you guys will take them , " which probably is also true .  , you know , it might be that people will take   th the algorithms apart and use the blocks from that . But I somehow think that it wouldn't be so bad , as long as people are happy abou    honest about it . professor b: And I think they have to be honest in the long run , because winning proposal again  what will be available th is will be a code . So the  the people can go to code and say " well listen this is what you stole from me " phd e:   The biggest problem of course is that f that Alcatel French Telecom cl claims " well we fulfilled the conditions . " And e and other people don't feel that , because they so they now decided that that is the whole thing will be done on well - endpointed data , essentially that somebody will endpoint the data based on clean speech , because most of this the SpeechDat - Car has the also close speaking mike and endpoints will be provided . professor b: And  we will run again still not clear if we are going to run the if we are allowed to run   new algorithms , but I assume so .  but since  u u n u at least our experience is that only endpointing a a mel cepstrum gets  gets you twenty - one percent improvement overall and twenty - seven improvement on SpeechDat - Car phd e:  . professor b: So they agreed that  there will be a twenty - five percent improvement required on on  h u m bad mis badly mismatched phd e: But wait a minute , I thought the endpointing really only helped in the noisy cases . professor b: Yeah but you have the same prob  MFCC basically has an enormous number of  insertions . professor b: And so , so now they want to say " we we will require fifty percent improvement only for well matched condition , and only twenty - five percent for the serial cases . professor b: And  and they almost agreed on that except that it wasn't a hundred percent agreed . And so last time  during the meeting , I just  brought up the issue , I said " well you know  quite frankly I 'm surprised how lightly you are making these decisions because this is a major decision . For two years we are fighting for fifty percent improvement and suddenly you are saying " oh no we we will do something less " , but maybe we should discuss that . And everybody said " oh we discussed that and you were not a mee there " and I said " well a lot of other people were not there because not everybody participates at these teleconferencing c things . " However , there is only ten or fifteen lines , so people can't even con you know participate . " Immediately Nokia  raised the question and they said " oh yeah we agree this is not good to to  dissolve the   the  the criterion . professor b: So now officially , Nokia is   complaining and said they they are looking for support ,  I think QualComm is  saying , too " we shouldn't abandon the fifty percent yet . professor b: Next Wednesday we are going to have  another  teleconferencing call , so we 'll see what  where it goes . phd e: So what about the issue of  the weights on the for the different systems , the well - matched , and medium - mismatched and professor b: Yeah , that 's what that 's a g very good  point , because David says " well you know we ca we can manipulate this number by choosing the right weights anyways . professor b:  yeah , if of course if you put a zero  weight zero on a mismatched condition , or highly mismatched then then you are done . So phd e: And they 're the staying the same ? professor b: Well , of course people will not like it . Now What is happening now is that I th I think that people try to match the criterion to solution . But it 's should happen at a point where everybody feels comfortable that we did all what we could . professor b: Basically , I think that that this test was a little bit bogus because of the data and  essentially there were these arbitrary decisions made , and and everything . So what we are doing at OGI now is    working basically on our parts which we I think a little bit neglected , like noise separation .  so we are looking in ways is in  which  with which we can provide better initial estimate of the mel spectrum basically , which would be a l  , f more robust to noise , and so far not much  success . professor b: We tried  things which  a long time ago Bill Byrne suggested , instead of using Fourier spectrum , from Fourier transform , use the spectrum from LPC model . Their argument there was the LPC model fits the peaks of the spectrum , so it may be m naturally more robust in noise . And I thought " well , that makes sense , " but so far we can't get much much out of it . professor b:  we may try some standard techniques like spectral subtraction and phd e: You haven't tried that yet ? professor b: not not not much . professor b: like for instance  Dennis Klatt was suggesting  the one way to  deal with noisy speech is to add noise to everything . phd e: Oh ! professor b: So that makes  th any additive noise less addi less a a effective , phd e: I see . It was kind of like one of these things , you know , but if you think about it , it 's actually pretty ingenious . So well , you know , just take a take a spectrum and and and add of the constant , C , to every every value . And if if then if this data becomes noisy , it b it becomes eff effectively becomes less noisy basically . professor b: But of course you cannot add too much noise because then you 'll s then you 're clean recognition goes down , but  it 's yet to be seen how much , it 's a very simple technique . professor b: Yes indeed it 's a very simple technique , you just take your spectrum and and use whatever is coming from FFT , add constant , phd e:  . That that Or the other thing is of course if you have a spectrum , what you can s start doing , you can leave start leaving out the p the parts which are   low in energy and then perhaps  one could try to find a a all - pole model to such a spectrum . Because a all - pole model will still try to to to put the the continuation basically of the of the model into these parts where the issue set to zero . phd e:  ! What is that ? professor b: Ah , you don't know about TRAPS ! grad a:  . phd e: The TRAPS sound familiar , I but I don't professor b: Yeah  tha This is familiar like sort of because we gave you the name , but , what it is , is that normally what you do is that you recognize  speech based on a shortened spectrum . professor b: Essentially L P - LPC , mel cepstrum , everything starts with a spectral slice .  so if you s So , given the spectrogram you essentially are sliding sliding the spectrogram along the  f frequency axis phd e:   professor b: So you can say " well you can also take the time trajectory of the energy at a given frequency " , and what you get is then , that you get a p vector . Namely you can say i it I will I will say that this vector will eh will will describe the phoneme which is in the center of the vector . professor b: And you so you classi so it 's a very different vector , very different properties , we don't know much about it , but the truth is phd e:  . But you have many of those vectors per phoneme , professor b: Well , so you get many decisions . professor b: Because if you run this  recognition , you get you still get about twenty percent error  twenty percent correct . professor b: on on like for the frame by frame basis , so   so it 's much better than chance . But  the latest  observation  is that you you you are you can get quite a big advantage of using two critical bands at the same time . Because there are some reasons I can I could talk about , will have to tell you about things like masking experiments which     yield critical bands , and also experiments with release of masking , which actually tell you that something is happening across critical bands , across bands . And phd e: Well how do you how do you  convert this  energy over time in a particular frequency band into a vector of numbers ? professor b: It 's     time T - zero is one number , time t phd e: Yeah but what 's the number ? Is it just the professor b: It 's a spectral energy , logarithmic spectral energy , phd e: it 's just the amount of energy in that band from f in that time interval . professor b: And that 's what that 's what I 'm saying then , so this is a this is a starting vector . professor b: for instance a question is like " how correlated are the elements of this vector ? " Turns out they are quite correlated , because  , especially the neighboring ones , right ? They they represent the same almost the same configuration of the vocal tract . professor b: Then the question is  " can you describe elements of this vector by Gaussian distributions " , or to what extent ? Because  And and and so on and so on . professor b: But  is the is the critical band the right   dimension ? So we somehow made arbitrary decision , " yes " . Then but then now we are thinking a lot how to  how to use at least the neighboring band because that seems to be happening This I somehow start to believe that 's what 's happening in recognition . Cuz a lot of experiments point to the fact that people can split the signal into critical bands , but then oh   so you can you are quite capable of processing a signal in   independently in individual critical bands . But at the same time you most likely pay attention to at least neighboring bands when you are making any decisions , you compare what 's happening in in this band to what 's happening to the band to to to the to the neighboring bands . That 's why the articulatory events , which  F F Fletcher talks about , they are about two critical bands . professor b: You need to you need to compare it to something else , what 's happening but it 's what 's happening in the in the close neighborhood . So if you are making decision what 's happening at one kilohertz , you want to know what 's happening at nine hundred hertz and it and maybe at eleven hundred hertz , but you don't much care what 's happening at three kilohertz . phd e: So it 's really w It 's sort of like saying that what 's happening at one kilohertz depends on what 's happening around it . But it 's but for but for instance , th   what what  humans are very much capable of doing is that if th if they are exactly the same thing happening in two neighboring critical bands , recognition can discard it . grad a: Hey ! professor b: Hey ! OK , we need us another another voice here . professor b: And so so so for instance if you d if you a if you add the noise that normally masks masks the  the the signal right ? phd e:   professor b: and you can show that in that if the if you add the noise outside the critical band , that doesn't affect the the decisions you 're making about a signal within a critical band . If the noise is modulated , with the same modulation frequency as the noise in a critical band , the amount of masking is less . professor b: So the s m masking curve , normally it looks like sort of I start from from here , so you you have  no noise then you you you are expanding the critical band , so the amount of maching is increasing . And when you e hit a certain point , which is a critical band , then the amount of masking is the same . professor b: But , if you if you if you modulate the noise , the masking goes up and the moment you start hitting the another critical band , the masking goes down . So essentially essentially that 's a very clear indication that that that cognition can take   into consideration what 's happening in the neighboring bands . But if you go too far in a in a if you if the noise is very broad , you are not increasing much more , so so if you if you are far away from the signal  from the signal f  the frequency at which the signal is , then the m even the when the noise is co - modulated it it 's not helping you much . So things like this we are kind of playing with with with the hope that perhaps we could eventually u use this in a in a real recognizer . phd e: But you probably won't have anything before the next time we have to evaluate , professor b: Probably not . phd e: right ? professor b: Well , maybe , most likely we will not have anything which c would comply with the rules . professor b: latency currently chops the require  significant  latency amount of processing , phd e:   professor b: because  we don't know any better , yet , than to use the neural net classifiers ,  and  and  TRAPS . professor b: Though the the work which  everybody is looking at now aims at s trying to find out what to do with these vectors , so that a g simple Gaussian classifier would be happier with it . professor b: or to what extent a Gaussian classifier should be unhappy  that , and how to Gaussian - ize the vectors , and phd e:  . Then Sunil is    asked me f for one month 's vacation and since he did not take any vacation for two years , I had no I didn't have heart to tell him no . professor b: And  phd e: Is he getting married or something ? professor b:  well , he may be looking for a girl , for for I don't I don't I don't ask . Well , I 've known other friends who they they go to Ind - they go back home to India for a month , they come back married , professor b: Yeah . professor b: and then of course then what happened with Narayanan was that he start pushing me that he needs to get a PHD because they wouldn't give him his wife . And she 's very pretty and he loves her and so so we had to really phd e: So he finally had some incentive to finish , professor b: Oh yeah . phd e: huh ? professor b: Sort of figured that That was a  that he  he told me the day when we did very well at our NIST evaluations of speaker recognition , the technology , and he was involved there . So I I said " well , yeah , OK " so he took another another three quarter of the year but  he was out . phd e: huh ? professor b: So I wouldn't surprise me if he has a plan like that , though though  Pratibha still needs to get out first . professor b: And S and Satya needs to get out very first because he 's he already has  four years served , though one year he was getting masters . phd e: So have the  when is the next  evaluation ? June or something ? professor b: Which ? Speaker recognition ? phd e: No , for  Aurora ? professor b:  there , we don't know about evaluation , next meeting is in June . But I  , yeah , what I think would be of course extremely useful , if we can come to our next meeting and say " well you know we did get fifty percent improvement . If if you are interested we eventually can tell you how " , but  we can get fifty percent improvement . Do you know what the new baseline is ? Oh , I guess if you don't have professor b: Twenty - two t twenty twenty - two percent better than the old baseline . But I assume that it will be similar , I don't I I don't see the reason why it shouldn't be . professor b: Cuz if it is worse , then we will raise the objection , phd e: Yeah . professor b: we say " well you know how come ? " Because eh if we just use our voice activity detector , which we don't claim even that it 's wonderful , it 's just like one of them . professor b: We get this sort of improvement , how come that we don't see it on on on on your endpointed data ? phd c: Yeah . phd c: because the voice activity detector that I choosed is something that cheating , it 's using the alignment of the speech recognition system , professor b: Yeah . C yeah  phd c: and only the alignment on the clean channel , and then mapped this alignment to the noisy channel . professor b: Well David told me David told me yesterday or Harry actually he told Harry from QualComm and Harry  brought up the suggestion we should still go for fifty percent he says are you aware that your system does only thirty percent  comparing to to endpointed baselines ? So they must have run already something . But  we think that we we didn't say the last word yet , that we have other other things which we can try . Because Nokia was objecting , with  QualComm 's we basically supported that , we said " yes " . "  The Guenter Hirsch who d doesn't speak for Ericsson anymore because he is not with Ericsson and Ericsson may not may withdraw from the whole Aurora activity because they have so many troubles now . phd e: Where 's  Guenter going ? professor b: Well Guenter is already he got the job  already was working on it for past two years or three years phd e:   professor b: he got a job  at some some Fachschule , the technical college not too far from Aachen . phd e:  ! professor b: So it 's like professor u university professor phd e:   professor b: you know , not quite a university , not quite a sort of it 's not Aachen University , but it 's a good school and he he 's happy .  ! professor b: And he well , he was hoping to work  with Ericsson like on t  like consulting basis , but right now he says says it doesn't look like that anybody is even thinking about speech recognition . But this is being now discussed right now , and it 's possible that  that that it may get through , that we will still stick to fifty percent . Which event es essentially I think that we should be happy with because that that would mean that at least people may be forced to look into alternative solutions phd c:   professor b: but not phd c: Which would mean like sixty percent over the current baseline , which is professor b: Yeah . professor b: Is it like sort of is How did you come up with this number ? If you improve twenty by twenty percent the c the f the all baselines , it 's just a quick c comp co computation ? phd c: Yeah . phd c: Yeah , because it de it depends on the weightings professor b: Yeah , yeah . How 's your documentation or whatever it w what was it you guys were working on last week ? phd c: Yeah , finally we we 've not finished with this . phd d: Ma - nec to need a little more time to improve the English , and maybe s to fill in something some small detail , something like that , phd c:   Well , we have a document that explain a big part of the experiments , phd d: Necessary to to include the bi the bibliography . phd e: So have you been running some new experiments ? I I thought I saw some jobs of yours running on some of the machine phd c: Yeah . We 've fff done some strange things like removing C - zero or C - one from the the vector of parameters , and we noticed that C - one is almost not useful at all . phd e: Eh Is this in the baseline ? or in  phd c: In the No , in the proposal . professor b: So we were just discussing , since you mentioned that , in it w phd c:   professor b: driving in the car with Morgan this morning , we were discussing a good experiment for b for beginning graduate student who wants to run a lot of who wants to get a lot of numbers on something phd c:   professor b: which is , like , " imagine that you will you will start putting every co any coefficient , which you are using in your vector , in some general power . professor b: So if you put it in a s square root , that effectively makes your model half as efficient . professor b: And and  i i i but it 's the mean is an exponent of the whatever , the the this Gaussian function . phd e: You 're compressing the range , professor b: So you 're compressing the range of this coefficient , so it 's becoming less efficient . Morgan was @ @ and he was he was saying well this might be the alternative way how to play with a with a fudge factor , you know ,  in the phd e: Oh . professor b: And I said " well in that case why don't we just start compressing individual elements , like when when because in old days we were doing when when people still were doing template matching and Euclidean distances , we were doing this liftering of parameters , right ? phd e:  - huh . professor b: and it 's highly affected by  frequency response of the of the recording equipment and that sort of thing , phd c:   professor b:  Bell Labs had he this   r raised cosine lifter which still I think is built into H HTK for reasons n unknown to anybody , but but  we had exponential lifter , or triangle lifter , basic number of lifters . But so they may be a way to to fiddle with the f with the f phd e: Insertions . professor b: Insertions , deletions , or the the giving a relative  basically modifying relative importance of the various parameters . professor b: The only of course problem is that there 's an infinite number of combinations and if the if you s if y phd e: Oh . You need like a some kind of a professor b: Yeah , you need a lot of graduate students , and a lot of computing power . phd e: You need to have a genetic algorithm , that basically tries random permutations of these things . If you were at Bell Labs or I d d I shouldn't be saying this in on on a mike , right ? Or I  IBM , that 's what maybe that 's what somebody would be doing . professor b: Oh ,  the places which have a lot of computing power , so because it is really it 's a p it 's a it 's it will be reasonable search phd e:   professor b:  but I wonder if there isn't some way of doing this  search like when we are searching say for best discriminants . phd e: You know actually , I don't know that this wouldn't be all that bad . phd e: right ? And then these exponents are just applied to that professor b: Absolutely . Each each phd e: And is this something that you would adjust for training ? or only recognition ? professor b: For both , you would have to do . phd e: So you 'd actually professor b: Because essentially you are saying "  this feature is not important " . professor b: Or less important , so that 's th that 's a that 's a painful one , yeah . phd e: So for each  set of exponents that you would try , it would require a training and a recognition ? professor b: Yeah . You just may n may need to c  give  less weight to to  a mod  a component of the model which represents this particular feature . So if you Instead of altering the feature vectors themselves , you you modify the the the Gaussians in the models . You modify the Gaussian in the model , but in the in the test data you would have to put it in the power , but in a training what you c in a training  in trained model , all you would have to do is to multiply a model by appropriate constant . But why if you 're if you 're multi if you 're altering the model , why w in the test data , why would you have to muck with the  cepstral coefficients ? professor b: Because in  test in  test data you ca don't have a model . professor b: That is true , but w  , so what you want to do You want to say if  obs you if you observe something like Stephane observes , that C - one is not important , you can do two things . professor b: If you have a trained trained recognizer , in the model , you know the the the the component which I  di dimension wh phd e:   All of the all of the mean and variances that correspond to C - one , you put them to zero . But what I 'm proposing now , if it is important but not as important , you multiply it by point one in a model . professor b: But but but phd e: But what are you multiplying ? Cuz those are means , right ? grad a: You 're multiplying the standard deviation ? phd e:  you 're grad a: So it 's professor b: I think that you multiply the I would I would have to look in the in the math ,  how how does the model  phd e: I think you professor b: Yeah . phd e: Yeah , I think you 'd have to modify the standard deviation or something , so that you make it wider or narrower . professor b: Effectively you , you know y in f in front of the of the model , you put a constant . grad a: right ? professor b: And and and phd e: So by making th the standard deviation narrower ,  your scores get worse for professor b: Yeah . By making it narrower , phd e: Right ? professor b:  y your phd e:  there 's you 're you 're allowing for less variance . Because see what you are fitting is the multidimensional Gaussian , right ? phd e:   professor b: It 's a it has it has  thirty - nine dimensions , or thirteen dimensions if you g ignore deltas and double - deltas . professor b: So in order if you in order to make dimension which which Stephane sees  less important ,    not not useful , less important , what you do is that this particular component in the model you can multiply by w you can you can basically de - weight it in the model . But you can't do it in a in a test data because you don't have a model for th   when the test comes , but what you can do is that you put this particular component in and and you compress it . phd e: Couldn't you just do that to the test data and not do anything with your training data ? professor b: That would be very bad , because  your t your model was trained  expecting  , that wouldn't work . After you train the model , you sort of y you could do you could do still what I was proposing initially , that during the training you you compress C - one that becomes then it becomes less important in a training . professor b: But if you have if you want to run e ex extensive experiment without retraining the model , you don't have to retrain the model . But after , you wh when you are doing this parametric study of importance of C - one you will de - weight the C - one component in the model , and you will put in the you will compress the this component in a in the test data . phd e: Could you also if you wanted to if you wanted to try an experiment  by leaving out say , C - one , couldn't you , in your test data ,  modify the all of the C - one values to be  way outside of the normal range of the Gaussian for C - one that was trained in the model ? So that effectively , the C - one never really contributes to the score ? phd c:   professor b: No , that would be a severe mismatch , phd e: Do you know what I 'm say professor b: right ? what you are proposing ? N no you don't want that . But what if you set if to the mean of the model , then ? And it was a cons you set all C - ones coming in through your test data , you you change whatever value that was there to the mean that your model had . professor b: I see what you are sa saying , phd c: Right ? grad a: Saying .   , no , the If you set it to a mean , that would No , you can't do that . phd e: Oh , that 's true , right , yeah , because you you have phd c: Wait . Which professor b: Because that would be a really f fiddling with the data , phd e: Yeah .  professor b: But what you can do , I 'm confident you ca phd e:  professor b: well , I 'm reasonably confident and I putting it on the record , right ?  y people will listen to it for for centuries now , is what you can do , is you train the model  with the with the original data . So what you will do is that a component in the model for C - one , you will divide it by by two . Then if you think that some component is more is more important then th th th it then then   i it is , based on training , then you  multiply this particular component in the model by by by phd e: You 're talking about the standard deviation ? professor b: yeah . professor b: Yeah , multiply this component  i it by number b larger than one , phd e:   phd c: Yeah , but , at the phd e: But don't you have to do something to the mean , also ? professor b: No . phd c: But I think it 's  the The variance is on on the denominator in the in the Gaussian equation . If you want to decrease the importance of a c parameter , you have to increase it 's variance . phd e: And now you 're you 're you 're changing that by squaring it . phd e: Do you see what  ? phd c: I think What I see What could be done is you don't change your features , which are computed once for all , professor b:  - huh . phd c: And then if you want to decrease the importance of C - one you just take the variance of the C - one component in the in the model and increase it if you want to decrease the importance of C - one or decrease it phd e: Yeah . Yeah , but  , but it 's it 's i it 's do - able , phd c: Well . phd e: to get  this  this the effect I think that you 're talking about , professor b:   phd e: Yeah , because if you had a huge variance , you 're dividing by a large number , you get a very small contribution . grad a: Doesn't matter phd c: Yeah , it becomes more flat grad a: Right . grad a: Yeah , the sharper the variance , the more more important to get that one right . phd e: Yeah , you know actually , this reminds me of something that happened  when I was at BBN . phd e: And this particular pitch algorithm  when it didn't think there was any voicing , was spitting out zeros . So we were getting  when we did clustering , we were getting groups  of features professor b: p Pretty new outliers , interesting outliers , right ? phd e: yeah , with with a mean of zero and basically zero variance . phd e: So , when ener when anytime any one of those vectors came in that had a zero in it , we got a great score . phd e: So if you have very small variance you get really good scores when you get something that matches . So that 's a way , yeah , yeah That 's a way to increase the yeah , n That 's interesting . phd e: You you have a step where you you modify the models , make a d copy of your models with whatever variance modifications you make , and rerun recognition . phd e: That could be set up fairly easily I think , and you have a whole bunch of you know professor b: Chuck is getting himself in trouble . Huh ! grad a: Didn't you say you got these  HTK 's set up on the new Linux boxes ? phd e: That 's right . professor b: Hey ! phd e: In fact , and and they 're just t right now they 're installing  increasing the memory on that  the Linux box . professor b: And Chuck is sort of really fishing for how to keep his computer busy , grad a: Right . professor b: that 's yeah , that 's a good thing grad a: That 's right . professor b: because then y you just write the " do " - loops and then you pretend that you are working while you are sort of you c you can go fishing . Then you are sort of in this mode like all of those ARPA people are , right ? phd e: Yeah . professor b: since it is on the record , I can't say  which company it was , but it was reported to me that  somebody visited a company and during a d during a discussion , there was this guy who was always hitting the carriage returns  on a computer . professor b: So after two hours  the visitor said " wh why are you hitting this carriage return ? " And he said " well you know , we are being paid by a computer ty  we are we have a government contract . " It was in old days when there were  of PDP - eights and that sort of thing . phd e: Oh , my gosh ! So he had to make it look like professor b: Because so they had a they literally had to c monitor at the time at the time on a computer how much time is being spent I i i or on on this particular project . phd e: Have you ever seen those little  It 's it 's this thing that 's the shape of a bird and it has a red ball and its beak dips into the water ? professor b: Yeah , I know , right . professor b: It would be similar similar to I knew some people who were  that was in old Communist  Czechoslovakia , right ? so we were watching for American airplanes , coming to spy on on  on us at the time , phd e:   professor b: so there were three guys   stationed in the middle of the woods on one l lonely  watching tower , pretty much spending a year and a half there because there was this service right ? And so they very quickly they made friends with local girls and local people in the village phd e: Ugh ! professor b: and phd e: Yeah . professor b: and so but they there was one plane flying over s always   above , and so that was the only work which they had . They like four in the afternoon they had to report there was a plane from Prague to Brno Basically f flying there , phd e: Yeah . professor b: so they f very q f first thing was that they would always run back and and at four o ' clock and and quickly make a call , " this plane is   passing " then a second thing was that they they took the line from this u u post to   a local pub . And they but third thing which they made , and when they screwed up , they finally they had to p the the p the pub owner to make these phone calls because they didn't even bother to be there anymore . At least they were sort of smart enough that they looked if the plane is flying there , right ? And the pub owner says " oh my four o ' clock , OK , quickly p pick up the phone , call that there 's a plane flying . professor b: There was no plane for some reason , phd e: And there wasn't ? professor b: it was downed , or and so they got in trouble . phd e: And we 'll just professor b: Well , at least go test the s test the  assumption about C - C - one  to begin with . It might be that  phd e: Yeah , so the first set of  variance weighting vectors would be just you know one modifying one and leaving the others the same . professor b: Because you see , what is happening here in a in a in a in such a model is that it 's tells you yeah what has a low variance  is  is  is more reliable , phd e: That would be one set of experiment professor b: right ? How do we phd e: Wh - yeah , when the data matches that , then you get really professor b: Yeah . professor b: How do we know , especially when it comes to noise ? phd e: But there could just naturally be low variance . professor b: Yeah ? phd e: Because I Like , I 've noticed in the higher cepstral coefficients , the numbers seem to get smaller , right ? So d phd c: They t phd e: just naturally . professor b: Yeah that 's why  people used these lifters were inverse variance weighting lifters basically that makes   Euclidean distance more like  Mahalanobis distance with a diagonal covariance when you knew what all the variances were over the old data . Turns out that  the variance decreases at least at fast , I believe , as the index of the cepstral coefficients . professor b: So typically what happens is that you you need to weight the  weight the higher coefficients more than  the lower coefficients . When we talked about Aurora still I wanted to m make a plea  encourage for  more communication between between   different  parts of the distributed   center .  even when there is absolutely nothing to to s to say but the weather is good in Ore - in in Berkeley . I 'm sure that it 's being appreciated in Oregon and maybe it will generate similar responses down here , like ,  phd c: We can set up a webcam maybe . phd e: Is the  if we mail to " Aurora - inhouse " , does that go up to you guys also ? professor b: I don't think so . professor b: We should definitely set up phd e: Yeah we sh Do we have a mailing list that includes  the OGI people ? professor b: Yeah . And then we also can send the the dis to the same address right , and it goes to everybody phd e:   professor b: Because what 's happening naturally in research , I know , is that people essentially start working on something and they don't want to be much bothered , right ? but what the the then the danger is in a group like this , is that two people are working on the same thing and i c of course both of them come with the s very good solution , but it could have been done somehow in half of the effort or something . reasonably  good one , because he 's doing it for Intel , but I trust that we have  rights to  use it  or distribute it and everything . phd e:  ! professor b: u s And so so  we we will make sure that at least you can see the software and if if if if it is of any use . He says " well you know it 's very difficult to collaborate if you are working with supposedly the same thing , in quotes , except which is not s is not the same . professor b: Which which   one is using that set of hurdles , another one set is using another set of hurdles . professor b: Yeah because Intel paid us  should I say on a microphone ?  some amount of money , not much . And they wanted  to to have software so that they can also play with it , which means that it has to be in a certain environment phd e:  . professor b: they use actu actually some Intel libraries , but in the process , Lucash just rewrote the whole thing because he figured rather than trying to f make sense  of  including ICSI software  not for training on the nets phd e:  . professor b: but I think he rewrote the the the or so maybe somehow reused over the parts of the thing so that so that the whole thing , including MLP , trained MLP is one piece of  software . professor b: Yeah ? grad a: I remember when we were trying to put together all the ICSI software for the submission . He said that it was like it was like just so many libraries and nobody knew what was used when , and and so that 's where he started and that 's where he realized that it needs to be needs to be   at least cleaned up , grad a: Yeah . Well , the the only thing I would check is if he does he use Intel math libraries , professor b:  e ev phd c: because if it 's the case , it 's maybe not so easy to use it on another architecture . professor b: n not maybe Maybe not in a first maybe not in a first ap approximation because I think he started first just with a plain C C or C - plus - plus or something before phd c: Ah yeah . I never checked carefully these sorts of professor b: I know there was some issues that initially of course we d do all the development on Linux but we use we don't have we have only three     s SUNs and we have them only because they have a SPERT board in . In that way Intel succeeded with us , because they gave us too many good machines for very little money or nothing . phd e: The way that it works is each person goes around in turn , and  you say the transcript number and then you read the digits , the the strings of numbers as individual digits . phd e: So you don't say " eight hundred and fifty " , you say " eight five oh " , and so forth 