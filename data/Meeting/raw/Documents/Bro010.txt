professor c: OK , what are we talking about today ? phd b: I don't know . phd a: Oh , this was the , talk where they were supposed to try to decide phd b: To to decide what to do , phd a: Ah , right .  , so  , let 's let 's assume for right now that we 're just kind of plugging on ahead , phd b: Yeah . professor c: because even if they tell us that , the rules are different , we 're still interested in doing what we 're doing .  , well , we 've a little bit worked on trying to see , what were the bugs and the problem with the latencies . phd d: To improve phd b: So , We took first we took the LDA filters and , we designed new filters , using  recursive filters actually . professor c: So when you say " we " , is that something Sunil is doing or is that ? phd b: I 'm sorry ? professor c: Who is doing that ? phd b: us . phd b: So we took the filters the FIR filters and we designed , IIR filters that have the same frequency response . phd b: So they had two filters , one for the low frequency bands and another for the high frequency bands . And the low frequency band has sixty - four milliseconds of delay , and the high frequency band filter has something like eleven milliseconds compared to the two hundred milliseconds of the IIR filters . So we have the filters but we still have to implement a routine that does recursive filtering professor c: OK . phd b: and professor c: You you had a discussion with Sunil about this though ? phd b: No . professor c: right ? So so you need to discuss with him what we 're doing , phd b: Yeah .  , I yeah , I don't know if th that 's what they were trying to They were trying to do something different like taking ,  well , using filter that takes only a past professor c: Right . I think that the the fact that we we did that with had that thing with the latencies was indicative of the fact that there wasn't enough communication . Well , there is w one , remark about these filters , that they don't have a linear phase . phd b: Well , I don't know , perhaps it perhaps it doesn't hurt because the phase is almost linear but .  , and so , yeah , for the delay I gave you here , it 's it 's , computed on the five hertz modulation frequency , which is the mmm , well , the most important for speech so . professor c: So that would be , a reduction of a hundred and thirty - six milliseconds , phd d: The low f f phd b: Yeah . professor c: which ,  What was the total we ended up with through the whole system ? phd b: Three hundred and thirty . professor c: So that would be within ? phd b: Yeah , but there are other points actually , which will perhaps add some more delay . Is that some other other stuff in the process were perhaps not very  perf well , not very correct , like the downsampling which w was simply dropping frames . phd b: so we will try also to add a nice downsampling having a filter that that professor c:  - huh .  , because wh when when we look at the LDA filters , well , they are basically low - pass but they leave a lot of what 's above twenty - five hertz . phd b: and so , yeah , this will be another filter which would add ten milliseconds again . phd b: yeah , and then there 's a third thing , is that , basically the way on - line normalization was done  , is just using this recursion on on the  , on the feature stream , professor c: Yeah .  , and when we look at this filter actually it has a delay of eighty - five milliseconds . If we want to be very correct , so if we want to the estimation of the mean t t to to be well , the right estimation of the mean , we have to t to take eighty - five milliseconds in the future . We would be at six so , sixty - five , plus ten , plus for the downsampling , plus eighty - five for the on - line normalization . So it 's professor c: phd b: plus plus eighty for the neural net and PCA . phd b: So it would be around two hundred and forty so , well , professor c: Just just barely in there . phd a: What 's the allowable ? professor c: Two - fifty , unless they changed the rules . phd a: What were they thinking of changing it to ? professor c: But phd b: Yeah . professor c: well the people who had very low latency want it to be low  , very very very narrow , latency bound . professor c: Unfortunately we 're the main ones with long latency , but phd a: Ah ! professor c: But , phd b: Yeah , and basically the best proposal had something like thirty or forty milliseconds of latency . professor c: Yeah , so they were basically  , they were more or less trading computation for performance and we were , trading latency for performance . And they were dealing with noise explicitly and we weren't , and so I think of it as complementary , that if we can put the phd a: Think of it as what ? professor c: Complementary . professor c: I think the best systems so , everything that we did in in a way it was it was just adamantly insisting on going in with a brain damaged system , which is something actually , we 've done a lot over the last thirteen years . So , w th w this was a test that largely had additive noise and we did we adde did absolutely nothing explicitly to handle ad additive noise . And , we did this , RASTA - like filtering which was done in the log domain and was tending to handle convolutional noise . So , the , spectral sub subtraction schemes a couple places did seem to seem to do a nice job . And so , we 're talking about putting putting some of that in while still keeping some of our stuff . I think you should be able to end up with a system that 's better than both but clearly the way that we 're operating for this other stuff does involved some latency to to get rid of most of that latency . To get down to forty or fifty milliseconds we 'd have to throw out most of what we 're doing . And and , I don't think there 's any good reason for it in the application actually .  , you 're you 're you 're speaking to a recognizer on a remote server and , having a a a quarter second for some processing to clean it up . professor c: These aren't large vocabulary things so the decoder shouldn't take a really long time , and . phd a: And I don't think anybody 's gonna notice the difference between a quarter of a second of latency and thirty milliseconds of latency . What what does wa was your experience when you were doing this stuff with , the the the surgical , microscopes and so forth .  , how long was it from when somebody , finished an utterance to when , something started happening ? phd a: we had a silence detector , so we would look for the end of an utterance based on the silence detector . phd a: And I I can't remember now off the top of my head how many frames of silence we had to detect before we would declare it to be the end of an utterance . phd a: but it was , I would say it was probably around the order of two hundred and fifty milliseconds . phd a: So professor c: Yeah , so you you so you had a phd a: this w professor c: so you had a a quarter second delay before , plus some little processing time , phd a: Right . professor c: And there 's physical inertia there , so probably the the motion itself was all phd a: And it felt to , the users that it was instantaneous . professor c: Yeah , so you would think as long as it 's under half a second or something . A person I don't think a person can tell the difference between , you know , a quarter of a second and a hundred milliseconds , and I 'm not even sure if we can tell the difference between a quarter of a second and half a second .  , basically if you yeah , if you said , " what 's the ,  what 's the shortest route to the opera ? " and it took half a second to get back to you , phd a: Yeah . phd a: because when we talk to each other we tend to step on each other 's utterances . So like if I 'm asking you a question , you may start answering before I 'm even done . Well , anyway , I think we could cut we know what else , we could cut down on the neural net time by by , playing around a little bit , going more into the past , or something like that . phd a: So is the latency from the neural net caused by how far ahead you 're looking ? professor c:   professor c: And there 's also well , there 's the neural net and there 's also this , multi - frame , KLT . phd a: Wasn't there Was it in the , recurrent neural nets where they weren't looking ahead at all ? professor c: They weren't looking ahead much . And and then But you also could just , we haven't experimented with this but I imagine you could , predict a , a label , from more in the past than in than than in the future . phd a: but I don't think professor c: Yeah , but we 've but we played a little bit with with asymmetric , guys . So , that 's what that 's what you 're busy with , s messing around with this , phd b: yeah . And , phd d: Also we were thinking to to , apply the eh , spectral subtraction from Ericsson phd b: Yeah . professor c: phd a: What is the advantage of that ? phd d:  phd b: Well , it 's that by the for the moment we have , something that 's discriminant and nonlinear . Well , it 's it 's a linear transformation , that  professor c: So at least just to understand maybe what the difference was between how much you were getting from just putting the frames together and how much you 're getting from the discriminative , what the nonlinearity does for you or doesn't do for you . Actually what we want to do , perhaps it 's to replace to to have something that 's discriminant but linear , also . And to see if it if it improves ov over over the non - discriminant linear transformation . professor c: Yeah , well , that 's what I meant , is to see whether whether it having the neural net really buys you anything . professor c: But maybe it 's just the discrimination and and maybe yeah , maybe the nonlinear discrimination isn't necessary . But the other part you were saying was the spectral subtraction , so you just kind of ,  phd b: Yeah . professor c: At what stage do you do that ? Do you you 're doing that ,  ? phd b: So it would be on the  on on the mel frequency bands , phd d: We was think phd b: so . professor c: OK , phd d: Yeah , professor c: so just do that on the mel f phd d: we no nnn We we was thinking to do before after VAD or phd b: Yeah , phd d: Oh , we don't know exactly when it 's better . phd b:  phd d: Before after VAD or professor c: So so you know that that that the way that they 're phd d: and then phd b:  . professor c: one thing that would be no good to find out about from this conference call is that what they were talking about , what they 're proposing doing , was having a third party , run a good VAD , and and determine boundaries . professor c: The reason for that was that ,  if some one p one group put in the VAD and another didn't , or one had a better VAD than the other since that they 're not viewing that as being part of the the task , and that any any manufacturer would put a bunch of effort into having some s kind of good speech - silence detection . It still wouldn't be perfect but  , e the argument was " let 's not have that be part of this test . " And so , I guess they argued about that yesterday and , yeah , I 'm sorry , I don't don't know the answer but we should find out . So ,  Yeah , so there 's the question of the VAD but otherwise it 's it 's on the the ,  the mel fil filter bank , energies I guess ? phd d:   professor c: And you 're you 're subtracting in the in the in the I guess it 's power power domain , or or magnitude domain . phd b: and professor c: if you look at the theory , it 's it should be in the power domain but but , I 've seen implementations where people do it in the magnitude domain phd b: Yeah . professor c: I have asked people why and they shrug their shoulders and say , " oh , it works . professor c: and there 's this I guess there 's this mysterious  people who do this a lot I guess have developed little tricks of the trade .  , there 's there 's this ,  you don't just subtract the the estimate of the noise spectrum . phd b: so you have the estimation of the power spectra of the noise , and you multiply this by a factor which is depend dependent on the SNR . phd a:  ! phd b: When the speech lev when the signal level is more important , compared to this noise level , the coefficient is small , and around one . But when the power le the s signal level is  small compared to the noise level , the coefficient is more important . And this reduce actually the music musical noise , phd a: Oh ! phd b:  which is more important during silence portions , phd a:  - huh . professor c: Well , that 's  , that 's what differs from different different tasks and different s  , spectral subtraction methods . phd a:  ! professor c: if if you have , fair assurance that , the noise is is quite stationary , then the smartest thing to do is use as much data as possible to estimate the noise , get a much better estimate , and subtract it off . professor c: But if it 's varying at all , which is gonna be the case for almost any real situation , you have to do it on - line , with some forgetting factor or something . phd a: So do you is there some long window that extends into the past over which you calculate the average ? professor c: Well , there 's a lot of different ways of computing the noise spectrum . So one of the things that , Hans - Guenter Hirsch did ,  and pas and other people actually , he 's he wasn't the only one I guess , was to , take some period of of of speech and in each band , develop a histogram . And , in fact I think the NIST standard method of determining signal - to - noise ratio is based on this . phd a: A couple seconds ? professor c: So No , no , it 's based on this kind of method , phd a:  . Now , if you have signal and you have noise , you basically have these two bumps in the histogram , which you could approximate as two Gaussians . professor c: So so basically now you have this mixture of two Gaussians , you you n know what they are , and , sorry , you estimate what they are , and , so this gives you what the signal is and what the noise e energy is in that band in the spectrum . And the other thing to do is which is sort of more trivial and obvious is to , determine through magical means that that , there 's no speech in some period , and then see what the spectrum is .  , a variant on that for just determining signal - to - noise ratio is to just ,  you can do a w a  an iterative thing , EM - like thing , to determine means only . professor c: And then you just use those mean values as being the the ,   signal - to - noise ratio in that band . phd a: But what is the it seems like this kind of thing could add to the latency . Cuz if you don't look into the future , right ? phd a: OK , well that I guess that was my question , professor c: if you just yeah phd a: yeah . professor c: if you just if you you ,  a at the beginning you have some phd a: Guess . professor c: esti some guess and and ,  phd b: Yeah , but it professor c: It 's an interesting question . I wonder how they did do it ? phd b: Actually , it 's a mmm If - if you want to have a good estimation on non - stationary noise you have to look in the in the future .  , if you take your window and build your histogram in this window , what you can expect is to have an estimation of th of the noise in in the middle of the window , not at the end . So professor c: Well , yeah , phd b: the but but people professor c: but what does what what what does Alcatel do ? phd d:   grad e: Pretty stationary , phd b: but ,  professor c: Well , the thing , e e e e grad e: yeah . professor c: Yeah , y  , you 're talking about non - stationary noise but I think that spectral subtraction is rarely is is not gonna work really well for for non - stationary noise , phd b: Well , if y if you have a good estimation of the noise , professor c: you know ? phd b: yeah , because well it it has to work . professor c: But it 's hard to phd b: i professor c: but that 's hard to do . So so I think that that what what is wh what 's more common is that you 're going to be helped with r slowly varying or stationary noise . professor c: If it varies a lot , to get a If if to get a good estimate you need a few seconds of speech , even if it 's centered , right ? phd b:   professor c: if you need a few seconds to get a decent estimate but it 's changed a lot in a few seconds , then it , you know , i it 's kind of a problem . professor c: imagine e five hertz is the middle of the of the speech modulation spectrum , phd b: Mmm . So , phd b: So in this case , yeah , sure , you cannot professor c: Yeah . phd b: But I think y  , Hirsch does experiment with windows of like between five hundred milliseconds and one second .  and he worked on non - stationary noises , like noise modulated with well , wi with amplitude modulations and things like that , phd a: Were his , windows centered around the phd b: and But  , yeah . professor c: No , I understand it 's better to do but I just think that that , for real noises wh what what 's most likely to happen is that there 'll be some things that are relatively stationary phd b: Mmm . professor c: and other things where it 's not so stationary and  , you can always pick something that that falls between your methods , phd b:  . professor c: but I don't know if , you know , if sinusoidally , modul amplitude modulated noise is is sort of a big problem in in in practice . professor c: I think that it 's  phd a: We could probably get a really good estimate of the noise if we just went to the noise files , and built the averages from them . phd b: What What do you mean ? professor c: Just cheat You 're saying , cheat . phd b: But if the if the noise is stationary perhaps you don't even need some kind of noise estimation algorithm . phd b: We just take th th th the beginning of the utterance and professor c: Oh , yeah , sure . phd b: Well , everybody seems to use some kind of adaptive , well , scheme professor c: But but phd d: Yeah . phd b: is it very useful professor c: you know , stationary phd a: Very slow adaptation . phd b: and is the c phd a: th professor c: Right , the word " stationary " is has a very precise statistical meaning . But , you know , in in signal - processing really what we 're talking about I think is things that change slowly , compared with our our processing techniques . professor c: So if you 're driving along in a car I I would think that most of the time the nature of the noise is going to change relatively slowly . If you if you check it out , five minutes later you may be in a different part of the road phd b:   But it 's it 's i i i using the local characteristics in time , is probably going to work pretty well . professor c: But you could get hurt a lot if you just took some something from the beginning of all the speech , of , you know , an hour of speech and then later phd b: Yeah . professor c: so they may be you know , may be overly , complicated for for this test but but but , I don't know .  , if possible you shouldn't you should you should make it , the center of the center of the window .  , phd a: If they 're going to provide a , voice activity detector that will tell you the boundaries of the speech , then , couldn't you just go outside those boundaries and do your estimate there ? professor c: Oh , yeah . So I I imagine that 's what they 're doing , right ? Is they 're they 're probably looking in nonspeech sections and getting some ,  phd b: Yeah , they have some kind of threshold on on the previous estimate , and So . Yeah , so , they h they have an estimate of the noise level and they put a threshold like six or ten DB above , and what 's under this threshold is used to update the estimate . It 's like saying what 's under the threshold is silence , professor c: Does France Telecom do this phd b: and grad e:  . professor c: Does France Telecom do th do the same thing ? More or less ? phd b: I d I Y you know , perhaps ? phd d: No .  , maybe we can talk about a couple other things briefly , just , things that that we 've been chatting about but haven't made it into these meetings yet . So you 're coming up with your quals proposal , and ,  Wanna just give a two three minute summary of what you 're planning on doing ? grad e: Oh , two , three , it can be shorter than that .  , but I 'm , looking into extending the work done by Larry Saul and John Allen and  Mazin Rahim .  , they they have a system that 's , a multi - band , system but their multi - band is is a little different than the way that we 've been doing multi - band in the past , where  Where we 've been @ @  taking  sub - band features and i training up these neural nets and on on phonetic targets , and then combining them some somehow down the line , they 're they 're taking sub - band features and , training up a detector that detects for , these phonetic features for example , he presents  , a detector to detect sonorance . And so what what it basically is is ,  it 's there 's at the lowest level , there it 's it 's an OR ga  , it 's an AND gate . So , on each sub - band you have several independent tests , to test whether  , there 's the existence of sonorance in a sub - band . And at the at the higher level , for every if ,  The higher level there 's a soft OR gate .  , so if if this detector detects  , the presence of of sonorance in any of the sub - bands , then the detect  , the OR gate at the top says , " OK , well this frame has evidence of sonorance . " phd a: What are what are some of the low level detectors that they use ? grad e: And these are all Oh , OK .  , and the ,  professor c: So that , by the way , basically is a is one of the units in our in our our neural network . Yeah , so he uses , an EM algorithm to to  train up these  parameters for the logistic regression . professor c: Well , actually , yeah , grad e: The professor c: so I was using EM to get the targets . So so you have this this this AND gate what we were calling an AND gate , but it 's a product product rule thing at the output . And then he uses , i u and then feeding into that are I 'm sorry , there 's it 's an OR at the output , isn't it ? Yeah , grad e:   phd a: And so are each of these , low level detectors are they ,  are these something that you decide ahead of time , like " I 'm going to look for this particular feature or I 'm going to look at this frequency , " or What what what are they looking at ? grad e:  phd a: What are their inputs ? grad e:  Right , so the OK , so at each for each sub - band there are basically , several measures of SNR and and correlation .  , and for for every s every sub - band , e you you just pick ahead of time , " I 'm going to have like five i independent logistic tests . grad e: And you initialize these parameters , in some some way and use EM to come up with your training targets for a for the the low - level detectors . grad e: And then , once you get that done , you you you train the whole whole thing on maximum likelihood .  , and h he shows that using this this method to detect sonorance is it 's very robust compared to ,  to typical , full - band Gaussian mixtures  estimations of of sonorance . You get enough of these detectors together , then you have enough information to do , higher level discrimination , for example , discriminating between phones phd a:   grad e: and then you keep working your way up until you you build a full recognizer . grad e: So , that 's that 's the direction which I 'm I 'm thinking about going in my quals .  , one is the going towards , using narrow band information for , ph phonetic features of some sort rather than just , immediately going for the the typical sound units . professor c: Another thing I like about it is that you t this thing is going to be trained explicitly trained for a product of errors rule , which is what , Allen keeps pointing out that Fletcher observed in the twenties , phd a:   And then , the third thing I like about it is , and we 've played around with this in a different kind of way a little bit but it hasn't been our dominant way of of operating anything , this issue of where the targets come from . So in our case when we 've been training it multi - band things , the way we get the targets for the individual bands is , that we get the phonetic label for the sound there phd a:   professor c: and we say , " OK , we train every " What this is saying is , OK , that 's maybe what our ultimate goal is or not ultimate but penultimate goal is getting these these small sound units . But but , along the way how much should we , what should we be training these intermediate things for ?  , because , we don't know  , that this is a particularly good feature .  , there 's no way ,  someone in the audience yesterday was asking , " well couldn't you have people go through and mark the individual bands and say where the where it was sonorant or not ? " phd a:   professor c: But , you know , I think having a bunch of people listening to critical band wide , chunks of speech trying to determine whether I think it 'd be impossible . professor c: It 's all gonna sound like like sine waves to you , more or less . professor c:  Well not  , it 's g all g narrow band  , i I m I think it 's very hard for someone to to a person to make that determination . It could sh be that you should , not be paying that much attention to , certain bands for certain sounds , in order to get the best result . professor c: So , what we have been doing there , just sort of mixing it all together , is certainly much much cruder than that . Now we have I guess done experiments you 've probably done stuff where you have , done separate , Viterbis on the different grad e: Yeah . Did did that help at all ? grad e: it helps for one or t one iteration but  , anything after that it doesn't help . professor c: So so that may or may t it that aspect of what he 's doing may or may not be helpful because in a sense that 's the same sort of thing . You 're taking global information and determining what you how you should But this is this is , I th I think a little more direct . phd a: How did they measure the performance of their detector ? professor c: And Well , he 's look he 's just actually looking at , the confusions between sonorant and non - sonorant . professor c: So he hasn't applied it to recognition or if he did he didn't talk about it . It 's it 's just And one of the concerns in the audience , actually , was that that , the ,  he he did a comparison to , you know , our old foil , the the nasty old standard recognizer with mel mel filter bank at the front , and H M Ms , and and so forth . But the one of the good questions in the audience was , well , yeah , but that wasn't trained for that .  , this use of a very smooth , spectral envelope is something that , you know , has evolved as being generally a good thing for speech recognition but if you knew that what you were gonna do is detect sonorants or not So sonorants and non - sonorants is is is almost like voiced - unvoiced , except I guess that the voiced stops are are also called " obstruents " .  , so it 's it 's  , but with the exception of the stops I guess it 's pretty much the same as voiced - unvoiced , phd a:   So , if you knew you were doing that , if you were doing something say for a a ,  a a Vocoder , you wouldn't use the same kind of features . Nonetheless , it was one that was interesting because , this is what we are actually using for speech recognition , these smooth envelopes . And this says that perhaps even , you know , trying to use them in the best way that we can , that that that we ordinarily do , with , you know , Gaussian mixtures and H M Ms and so forth , you you don't , actually do that well on determining whether something is sonorant or not . phd a: Didn't they professor c: Which means you 're gonna make errors between similar sounds that are son sonorant or obstruent . phd a: Didn't they also do some kind of an oracle experiment where they said " if we could detect the sonorants perfectly and then show how it would improve speech recognition ? I thought I remember hearing about an experiment like that . professor c: That would that 's you 're right , that 's exactly the question to follow up this discussion , is suppose you did that , got that right . phd b: What could be the other low level detectors , for Other kind of features , or ? in addition to detecting sonorants or ? Th - that 's what you want to to to go for also grad e:  phd b: or ? grad e: What t Oh , build other other detectors on different phonetic features ? phd b: Other low level detectors ? Yeah . e  , w easiest thing would be to go go do some voicing stuff but that 's very similar to sonorance . grad e: phd a: When we when we talked with John Ohala the other day we made a list of some of the things that w grad e: Yeah . professor c: Yeah , so there 's a half dozen like that that are grad e: Yeah , nasality . professor c: Now this was coming at it from a different angle but maybe it 's a good way to start .  , these are things which , John felt that a a ,  a human annotator would be able to reliably mark . So the sort of things he felt would be difficult for a human annotator to reliably mark would be tongue position kinds of things . professor c: But stress doesn't , fit in this thing of coming up with features that will distinguish words from one another , grad e:   professor c: right ? It 's a it 's a good thing to mark and will probably help us ultimate with recognition phd a: Yeah , there 's a few cases where it can like permit and permit . professor c: Well , yeah , but i either case you 'd write PERMIT , right ? So you 'd get the word right . phd a: No , I 'm saying , i i e I thought you were saying that stress doesn't help you distinguish between words . As long as you get The sequence , professor c: We 're g if we 're doing if we 're talking about transcription as opposed to something else phd a: right ? Yeah . Yeah , so that 's yeah , that 's , you know , a neat neat thing and and ,  So . grad e: S so , Ohala 's going to help do these ,  transcriptions of the meeting data ? phd a: well I don't know .  , we just talked about some possible features that could be marked by humans and , grad e:  . phd a: because of having maybe some extra transcriber time we thought we could go through and mark some portion of the data for that . professor c: that 's not an immediate problem , that we don't immediately have a lot of extra transcriber time . professor c: But but , in the long term I guess Chuck is gonna continue the dialogue with John and and , and , we 'll we 'll end up doing some I think .  , so , y you want to talk maybe a c two or three minutes about what we 've been talking about today and other days ? grad f: Ri Yeah , OK , so , we 're interested in , methods for far mike speech recognition , mainly , methods that deal with the reverberation in the far mike signal . So , one approach would be , say MSG and PLP , like was used in Aurora one and , there are other approaches which actually attempt to remove the reverberation , instead of being robust to it like MSG . And so we 're interested in , comparing the performance of  , a robust approach like MSG with these , speech enhancement or de - reverber de - reverberation approaches . grad f: And , it looks like we 're gonna use the Meeting Recorder digits data for that . phd b: And the de - reverberation algorithm , do you have can you give some more details on this or ? Does it use one microphone ? grad f: o o phd b: Several microphones ? Does it ? grad f: OK , well , there was something that was done by , a guy named Carlos , I forget his last name , who worked with Hynek , who , professor c: Avendano . grad f: it was like RASTA in the sense that of it was , de - convolution by filtering  , except he used a longer time window , phd b:   And the reason for that is RASTA 's time window is too short to ,  include the whole , reverberation  , I don't know what you call it the reverberation response . The reverberation filter from my mouth to that mike is like it 's t got it 's too long in the in the time domain for the  for the RASTA filtering to take care of it . And , then there are a couple of other speech enhancement approaches which haven't been tried for speech recognition yet but have just been tried for enhancement , which , have the assumption that  , you can do LPC  analysis of th of the signal you get at the far microphone and the , all pole filter that you get out of that should be good . It 's just the , excitation signal that is going to be distorted by the reverberation and so you can try and reconstruct a better excitation signal and , feed that through the i  , all pole filter and get enhanced speech with reverberation reduced . professor c: There 's also this , echo cancellation stuff that we 've sort of been chasing , so ,  we have ,  and when we 're saying these digits now we do have a close microphone signal and then there 's the distant microphone signal . And you could as a kind of baseline say , " OK , given that we have both of these , we should be able to do , a cancellation . " So that , we we , essentially identify the system in between the linear time invariant system between the microphones and and and and re and invert it , or or cancel it out to to some some reasonable approximation phd b: that 's not a practical thing , if you have a distant mike , you don't have a close mike ordinarily , but we thought that might make also might make a good baseline .  , but And then there are s  , there are single microphone methods that I think people have done for ,  for this kind of de - reverberation . Do y do you know any references to any ? Cuz I I w I was w w I I lead him down a a bad path on that . phd b: I g I guess I guess when people are working with single microphones , they are more trying to do professor c: But . phd b: well , not not very Well , there is the Avendano work , professor c: Right . phd b: but also trying to mmm ,  trying to f t find the de - convolution filter but in the  not in the time domain but in the  the stream of features  I guess . Well , @ @ there there 's someone working on this on i in Mons professor c: Yeah , OK . phd b: So perhaps , yeah , we should try t to He 's working on this , on trying to professor c: Yeah . phd b: on re reverberation ,  professor c: The first paper on this is gonna have great references , I can tell already . professor c: It 's always good to have references , especially when reviewers read it or or one of the authors and , feel they 'll " You 're OK , you 've r You cited me . Well , he did echo cancellation and he did some fancier things like , training different network on different reverberation conditions and then trying to find the best one , but . professor c: The oth the other thing , that Dave was talking about earlier was , multiple mike things , where they 're all distant . So , there 's there 's all this work on arrays , but the other thing is , what can we do that 's cleverer that can take some advantage of only two mikes , particularly if there 's an obstruction between them , as we as we have over there . It 's part of why you have such good directionality with , with two ears phd b:   professor c: So that Yeah , the the head , in the way , is really that 's what it 's for . It 's basically , phd a: That 's what the head 's for ? To separate the ears ? professor c: Yeah , it 's to separate the ears . professor c: Yeah ? phd a: I think th that may be due to the fact that Adam ran out of digits , and didn't have time to regenerate any . professor c: Oh ! Oh ! I guess it 's Well there 's no real reason to write our names on here then , phd a: Yeah , if you want to put your credit card numbers and ,  professor c: is there ? grad e: Oh , no ? professor c: Or do did any do we need the names for the other stuff , phd a: yeah , I do need your names and and the time , and all that , professor c: or ? Oh , OK 