grad b: So I guess this is more or less now just to get you up to date , Johno .  , so we thought that , We can write up  , an element , and for each of the situation nodes that we observed in the Bayes - net ? So . What 's the situation like at the entity that is mentioned ? if we know anything about it ? Is it under construction ? Or is it on fire or something happening to it ? Or is it stable ? and so forth , going all the way  , f through Parking , Location , Hotel , Car , Restroom , @ @ Riots , Fairs , Strikes , or Disasters . grad c: So is This is A situation are is all the things which can be happening right now ? Or , what is the situation type ? grad b: That 's basically just specifying the the input for the w what 's grad c: Oh , I see y Why are you specifying it in XML ? grad b:  . I just don't know if this is th l what the Does This is what Java Bayes takes ? as a Bayes - net spec ? grad b: No , because  if we  we 're sure gonna interface to We 're gonna get an XML document from somewhere . Right ? And that XML document will say " We are able to We were able to observe that w the element , @ @ of the Location that the car is near . grad b: So this is just , again , a an XML schemata which defines a set of possible , permissible XML structures , which we view as input into the Bayes - net . Right ? grad c: And then we can r  possibly run one of them  transformations ? That put it into the format that the Bayes n or Java Bayes or whatever wants ? grad b: Yea - Are you talking are you talking about the the structure ? grad c: Well it grad b:  when you observe a node . grad c: When you when you say the input to the v Java Bayes , it takes a certain format , grad b:   grad c: So you could just Couldn't you just run a grad b: XSL . grad b: That 's That 's no problem , but I even think that , once Once you have this sort of as running as a module Right ? What you want is You wanna say , " OK , give me the posterior probabilities of the Go - there node , when this is happening . " Right ? When the person said this , the car is there , it 's raining , and this is happening . And with this you can specify the what 's happening in the situation , and what 's happening with the user . So , this is a grad c: So this is just a specification of all the possible inputs ? grad b: Yep . So , we have , for example , the , Go - there decision node grad c: OK . grad b: which has two elements , going - there and its posterior probability , and not - going - there and its posterior probability , because the output is always gonna be all the decision nodes and all the the a all the posterior probabilities for all the values . grad c: And then we would just look at the , eh , Struct that we wanna look at in terms of if if we 're only asking about one of the So like , if I 'm just interested in the going - there node , I would just pull that information out of the Struct that gets return that would that Java Bayes would output ? grad b: pretty much , yes , but I think it 's a little bit more complex . As , if I understand it correctly , it always gives you all the posterior probabilities for all the values of all decision nodes . So , when we input something , we always get the , posterior probabilities for all of these . grad b: So there is no way of telling it t not to tell us about the EVA values . grad c: Yeah , wait I agree , that 's yeah , use oh ,  Yeah , OK . grad b: So so we get this whole list of of , things , and the question is what to do with it , what to hand on , how to interpret it , in a sense . So y you said if you " I 'm only interested in whether he wants to go there or not " , then I just look at that node , look which one grad c: Look at that Struct in the output , grad b: Yep . grad c: right ? grad b: Look at that Struct in the the output , even though I wouldn't call it a " Struct " . grad c: Well i well , it 's an XML Structure that 's being res returned , grad b: Oh . grad c: right ? grad b: So every part of a structure is a " Struct " . grad c: Yeah , I just  I just was abbreviated it to Struct in my head , and started going with that . And , the reason is why I think it 's a little bit more complex or why why we can even think about it as an interesting problem in and of itself is  . grad c: Well , w wouldn't we just take the structure that 's outputted and then run another transformation on it , that would just dump the one that we wanted out ? grad b: Yeah . grad b: No grad c: D Can't you just look at one specific grad b: Yeah , exactly . The @ @ Xerxes allows you to say , u " Just give me the value of that , and that , and that . " But , we don't really know what we 're interested in before we look at the complete at at the overall result . So the person said , " Where is X ? " and so , we want to know , is Does he want info ? o on this ? or know the location ? Or does he want to go there ? Let 's assume this is our our question . It 's always gonna give us a value of how likely we think i it is that he wants to go there and doesn't want to go there , or how likely it is that he wants to get information . So , does he wanna know where it is ? or does he wanna go there ? grad c: He wants to know where it is . And if it 's If grad c: Well now , y  , you could grad b: And i if there 's sort of a clear winner here , and ,  and this is pretty ,  indifferent , then we then we might conclude that he actually wants to just know where ,  t  , he does want to go there . grad c: out of curiosity , is there a reason why we wouldn't combine these three nodes ? into one smaller subnet ? that would just basically be the question for We have " where is X ? " is the question , right ? That would just be Info - on or Location ? Based upon grad b: Or Go - there . People come up to you on campus and say , " Where 's the library ? " You 're gonna say y you 're gonna say , g " Go down that way . " You 're not gonna say " It 's It 's five hundred yards away from you " or " It 's north of you " , or " it 's located " grad c: Well ,  But the there 's So you just have three decisions for the final node , that would link thes these three nodes in the net together . Again , in this Given this input , we , also in some situations , may wanna postulate an opinion whether that person wants to go there now the nicest way , use a cab , or so s wants to know it wants to know where it is because he wants something fixed there , because he wants to visit t it or whatever . So , it n  a All I 'm saying is , whatever our input is , we 're always gonna get the full output . grad c: But  , I guess I guess the thing is , this is another , smaller , case of reasoning in the case of an uncertainty , which makes me think Bayes - net should be the way to solve these things . So if you had If for every construction , grad b: Oh ! grad c: right ? you could say , " Well , there Here 's the Where - Is construction . " And for the Where - Is construction , we know we need to l look at this node , that merges these three things together grad b:   And since we have a finite number of constructions that we can deal with , we could have a finite number of nodes . grad c: Say , if we had to y deal with arbitrary language , it wouldn't make any sense to do that , because there 'd be no way to generate the nodes for every possible sentence . grad c: But since we can only deal with a finite amount of stuff grad b: So , basically , the idea is to f to feed the output of that belief - net into another belief - net . grad c: Yeah , so basically take these three things and then put them into another belief - net . grad b: But , why why why only those three ? Why not the whol grad c: Well , d For the Where - Is question . But we believe that all the decision nodes are can be relevant for the Where - Is , and the Where How - do - I - get - to or the Tell - me - something - about . Well , I do I See , I don't know if this is a good idea or not . But  , it seems like we could have I mea or  we could put all of the all of the r information that could also be relevant into the Where - Is node answer grad b:   grad b:  Let 's not forget we 're gonna get some very strong input from these sub dis from these discourse things , right ? So . " Nuh ? Or " Where is X located at ? " grad c: We u grad b: Nuh ? grad c: Yeah , I know , but the Bayes - net would be able to The weights on the on the nodes in the Bayes - net would be able to do all that , grad b:   grad c: wouldn't it ? Here 's a k Oh ! Oh , I 'll wait until you 're plugged in . The headphone that you have to put on backwards , with the little little thing and the little little foam block on it ? It 's a painful , painful microphone . grad c: The crown ? grad d: What ? grad b: Yeah , versus " the Sony " . grad b: You 're on - line ? grad c: Are you are your mike o Is your mike on ? grad a: Indeed . So you 've been working with these guys ? You know what 's going on ? grad a: Yes , I have . s So where are we ? grad c: Excellent ! grad b: We 're discussing this . A person says , " Where is X ? " , and we get a certain We have a Situation vector and a User vector and everything is fine ? An - an and and our and our grad c: Did you just sti Did you just stick the m the the the microphone actually in the tea ? grad a: No . grad b: let 's just assume our Bayes - net just has three decision nodes for the time being . These three , he wants to know something about it , he wants to know where it is , he wants to go there . grad c: In terms of , these would be wha how we would answer the question Where - Is , right ? We u This is i That 's what you s it seemed like , explained it to me earlier grad b: Yeah , but , mmm . grad c: w We we 're we wanna know how to answer the question " Where is X ? " grad b: Yeah . " grad c: Well , yeah , but in the s  , let 's just deal with the s the simple case of we 're not worrying about timing or anything . We just want to know how we should answer " Where is X ? " grad b: OK . And , OK , and , Go - there has two values , right ? , Go - there and not - Go - there . So , he wants to know something about it , and he wants to know something he wants to know Where - it - is , grad a: Excuse me . grad b: And , in this case we would probably all agree that he wants to go there . grad b: In the , whatever , if we have something like this here , and this like that and maybe here also some grad a: You should probably make them out of Yeah . grad b: something like that , grad c: Well , it grad b: then we would guess , " Aha ! He , our belief - net , has s stronger beliefs that he wants to know where it is , than actually wants to go there . " Right ? grad c: That it Doesn't this assume , though , that they 're evenly weighted ? grad d: True . grad a: The different decision nodes , you mean ? grad c: Yeah , the Go - there , the Info - on , and the Location ? grad a: Well , d yeah , this is making the assumption . grad c: Like grad b: What do you mean by " differently weighted " ? They don't feed into anything really anymore . grad a: But  , why do we grad c: Or I jus grad a: If we trusted the Go - there node more th much more than we trusted the other ones , then we would conclude , even in this situation , that he wanted to go there . grad c: Le grad a: So , in that sense , we weight them equally right now . But grad c: So the But I guess the k the question that I was as er wondering or maybe Robert was proposing to me is How do we d make the decision on as to which one to listen to ? grad a: Yeah , so , the final d decision is the combination of these three . So again , it 's it 's some kind of ,  grad c: Bayes - net . grad c: OK so , then , the question i So then my question is t to you then , would be So is the only r reason we can make all these smaller Bayes - nets , because we know we can only deal with a finite set of constructions ? Cuz oth If we 're just taking arbitrary language in , we couldn't have a node for every possible question , you know ? grad a: A decision node for every possible question , you mean ? grad c: Well , I like , in the case of Yeah . In the ca Any piece of language , we wouldn't be able to answer it with this system , b if we just h Cuz we wouldn't have the correct node . Basically , w what you 're s proposing is a n Where - Is node , right ? grad a: Yeah . grad c: And and if we And if someone says , you know , something in Mandarin to the system , we 'd - wouldn't know which node to look at to answer that question , grad a: So is Yeah . grad c: right ? grad b: Mmm ? grad c: So , but but if we have a finite What ? grad b: I don't see your point . What what what I am thinking , or what we 're about to propose here is we 're always gonna get the whole list of values and their posterior probabilities . And now we need an expert system or belief - net or something that interprets that , that looks at all the values and says , " The winner is Timing . Wh - Regardle grad c: Yeah , but But how does the expert but how does the expert system know how who which one to declare the winner , if it doesn't know the question it is , and how that question should be answered ? grad b: Based on the k what the question was , so what the discourse , the ontology , the situation and the user model gave us , we came up with these values for these decisions . But how do we weight what we get out ? As , which one i Which ones are important ? So my i So , if we were to it with a Bayes - net , we 'd have to have a node for every question that we knew how to deal with , that would take all of the inputs and weight them appropriately for that question . grad c: Does that make sense ? Yay , nay ? grad a: are you saying that , what happens if you try to scale this up to the situation , or are we just dealing with arbitrary language ? grad c: We grad a: Is that your point ? grad c: Well , no . Are we going to make a node for every question ? Does that make sense ? grad a: For every question ? grad c: Or not .  , it 's not based on constructions , it 's based on things like , there 's gonna be a node for Go - there or not , and there 's gonna be a node for Enter , View , Approach . grad c: How do we decide how to answer it ? grad b: Well , look at look Face yourself with this pr question . What do we think ? What does this tell us ? And not knowing what was asked , and what happened , and whether the person was a tourist or a local , because all of these factors have presumably already gone into making these posterior probabilities . What what we need is a just a mechanism that says , " Aha ! There is " grad c: Yeah . I just don't think a " winner - take - all " type of thing is the grad a: in general , like , we won't just have those three , right ? We 'll have , like , many , many nodes . So we have to , like So that it 's no longer possible to just look at the nodes themselves and figure out what the person is trying to say . So if if for example , the Go - there posterior possibility is so high , w if it 's if it has reached reached a certain height , then all of this becomes irrelevant . If even if if the function or the history or something is scoring pretty good on the true node , true value grad c: Wel I don't know about that , cuz that would suggest that  grad b: He wants to go there and know something about it ? grad c: Do they have to be mutual Yeah . grad c: Cuz I ,  The way you describe what they meant , they weren't mutu  , they didn't seem mutually exclusive to me . grad b: Well , if he doesn't want to go there , even if the Enter posterior proba So . grad c: Well , yeah , just out of the other three , though , that you had in the grad b:  ? grad c: those three nodes . But It 's through the grad c: So th s so , yeah , but some So , some things would drop out , and some things would still be important . grad c: But I guess what 's confusing me is , if we have a Bayes - net to deal w another Bayes - net to deal with this stuff , grad a:   grad c: you know , is the only reason OK , so , I guess , if we have a Ba - another Bayes - net to deal with this stuff , the only r reason we can design it is cuz we know what each question is asking ? grad a: Yeah . grad c: And then , so , the only reason way we would know what question he 's asking is based upon Oh , so if Let 's say I had a construction parser , and I plug this in , I would know what each construction the communicative intent of the construction was grad a:   So no matter what they said , if I could map it onto a Where - Is construction , I could say , " ah ! grad a: Ge   grad c: well the the intent , here , was Where - Is " , grad a: OK , right . Yeah , I 'm also agreeing that a simple pru Take the ones where we have a clear winner . Right ? But in this case if we say , " definitely he doesn't want to go there . " or let 's call this this " Look - At - H " He wants to know something about the history of . " Now , the e But for some reason the Endpoint - Approach gets a really high score , too . We can't expect this to be sort of at O point three , three , three , O point , three , three , three , O point , three , three , three . You know ? Or know There needs to be some knowledge that grad c: We Yeah , but , the Bayes - net that would merge I just realized that I had my hand in between my mouth and my micr er , my and my microphone . So then , the Bayes - net that would merge there , that would make the decision between Go - there , Info - on , and Location , would have a node to tell you which one of those three you wanted , and based upon that node , then you would look at the other stuff . It 's sort of one of those , that 's It 's more like a decision tree , if if you want . You first look o at the lowball ones , grad c: Yeah , i grad b: and then grad c: Yeah , I didn't intend to say that every possible OK . There was a confusion there , k I didn't intend to say every possible thing should go into the Bayes - net , because some of the things aren't relevant in the Bayes - net for a specific question . Like the Endpoint is not necessarily relevant in the Bayes - net for Where - Is until after you 've decided whether you wanna go there or not .  , when you 're asked a specific question and you don't even Like , if you 're asked a Where - Is question , you may not even look like , ask for the posterior probability of the , EVA node , right ? Cuz , that 's what  , in the Bayes - net you always ask for the posterior probability of a specific node . You can compute , the posterior probability of one subset of the nodes , given some other nodes , but totally ignore some other nodes , also . So you have to make grad b: " OK , if it 's a Where - Is question , which decision nodes do I query ? " grad a: Yeah . grad d: So it 's pretty much the same problem , grad b: Yeah it 's it 's it 's apples and oranges . grad d: isn't it ? grad b: Nuh ?  , maybe it does make a difference in terms of performance , computational time . grad b: So either you always have it compute all the posterior possibilities for all the values for all nodes , and then prune the ones you think that are irrelevant , grad a: Mmm . grad b: or you just make a p @ @ a priori estimate of what you think might be relevant and query those . And just basically do a binary search through the ? grad a: I don't know if it would necessarily be that , complicated . But , it w grad c: Well , in the case of Go - there , it would be . In the case Cuz if you needed an If y If Go - there was true , you 'd wanna know what endpoint was . And if it was false , you 'd wanna d look at either Lo - Income Info - on or History . grad a: OK , why 's that ? grad c: I can't figure out how to get the probabilities into it . grad c: Ju grad a: It 's there 's a grad c: Oh yeah , yeah . I d I just think I haven't figured out what the terms in Hugin mean , versus what Java Bayes terms are . grad b: by the way , are Do we know whether Jerry and Nancy are coming ? grad a: So we can figure this out . grad b: Or ? grad a: They should come when they 're done their stuff , basically , whenever that is . grad c: What d what do they need to do left ? grad a: I guess , Jerry needs to enter marks , but I don't know if he 's gonna do that now or later . But , if he 's gonna enter marks , it 's gonna take him awhile , I guess , and he won't be here . grad c: And what 's Nancy doing ? grad a: Nancy ?  , she was sorta finishing up the , calculation of marks and assigning of grades , but I don't know if she should be here . grad c: She 's on the email list , right ? grad a: Is she ? OK . Because basically , what where we also have decided , prior to this meeting is that we would have a rerun of the three of us sitting together grad d: OK . grad c: Well , I grad d: You added a bunch of nodes , for ? grad b: Yep . grad b: Right ? grad c: what do the , structures do ? grad b:  ? grad c: So the the the For instance , this Location node 's got two inputs , grad a: Four inputs . grad c: Cuz I thought it was like , that one in Stuart 's book about , you know , the grad a: Alarm in the dog ? grad c: U Yeah . grad c: Yeah , there 's a dog one , too , but that 's in Java Bayes , grad a: Right . grad b: And we have all the top ones , all the ones to which no arrows are pointing . What we 're missing are the these , where arrows are pointing , where we 're combining top ones . So , we have to come up with values for this , and this , this , this , and so forth . grad c: Cuz of Memorial Day ? grad a: We 'll meet next Tuesday , I guess . grad c: Or , three days ? grad a: Is he How long is he gone for ? grad b: Two weeks . grad a: Italy , huh ? What 's ,  what 's there ? grad b: Well , it 's a country . Part of what we actually want to do is sort of schedule out what we want to surprise him with when when he comes back . grad b: Yeah ? You or have a finished construction parser and a working belief - net , and  grad c: That wouldn't be disappointing . I had I I had sort of scheduled out in my mind that you guys do a lot of work , and I do nothing . But , i do you guys have any vacation plans , because I myself am going to be , gone , but this is actually not really important . But we 're all going to be here on Tuesday again ? Looks like it ? grad d: Yeah . And once we have finished it , I guess we can ,  and that 's going to be more just you and me , because Bhaskara is doing probabilistic , recursive , structured , object - oriented , grad c: Killing machines ! grad b: reasoning machines . So you 're saying , next Tuesday , is it the whole group meeting , or just us three working on it , or or ? grad b:  . grad b: definite grad d: So , when you were saying we need to do a re - run of , like grad a: h What ? grad d: What Like , just working out the rest of the grad b: Yeah . grad c: When you say , " the whole group " , you mean the four of us , and Keith ? grad d: OK . grad c: Ami might be here , and it 's possible that Nancy 'll be here ? grad b: Yep . grad b: Because , th you know , once we have the belief - net done grad c: You 're just gonna have to explain it to me , then , on Tuesday , how it 's all gonna work out . Because then , once we have it sort of up and running , then we can start you know , defining the interfaces and then feed stuff into it and get stuff out of it , and then hook it up to some fake construction parser and grad c: That you will have in about nine months or so . grad b: And , grad c: The first bad version 'll be done in nine months . grad b: Yeah , I can worry about the ontology interface and you can Keith can worry about the discourse .  , this is pretty  , I I I hope everybody  knows that these are just going to be  dummy values , right ? grad a: Which grad b: where the grad a: Which ones ? grad b: S so so if the endpoint if the Go - there is Yes and No , then Go - there - discourse will just be fifty - fifty . Right ? grad a: what do you mean ? If the Go - there says No , then the Go - there is grad d: I don't get it . grad b: But , what are the values of the Go - there - discourse ? grad a: Well , it depends on the situation . If the discourse is strongly indicating that grad b: Yeah , but , we have no discourse input . grad d: So , so far we have Is that what the Keith node is ? grad b: Yep . And you 're taking it out ? for now ? grad b: Well , this is D grad d: Or ? grad b: OK , this , I can I can get it in here . grad d: All the D 's are grad b: I can get it in here , so th We have the , sk let 's let 's call it " Keith - Johno grad a: Johno ? grad b: node " . grad b: And , grad c: Does th th does the H go b before the A or after the A ? grad a: Oh , in my name ? Before the A . Cuz you kn When you said people have the same problem , I thought Cuz my H goes after the  e e e the v grad a: People have the inverse problem with my name . I always have to check , every time y I send you an email , a past email of yours , to make sure I 'm spelling your name correctly . grad b: But , when you abbreviate yourself as the " Basman " , you don't use any H 's . grad a: " Basman " ? Yeah , it 's because of the chessplayer named Michael Basman , who is my hero . grad c: How do you pronou How do you pronounce your name ? grad d: Eva . grad c: What if I were What if I were to call you Eva ? grad d: I 'd probably still respond to it . Like if I u take the V and s pronounce it like it was a German V ? grad b: Which is F . grad d: I grad c: There 's also an F in German , grad d: OK . It doesn't matter what those nodes are , anyway , because we 'll just make the weights " zero " for now . We 'll make them zero for now , because it who who knows what they come up with , what 's gonna come in there . grad c: Wait , maybe it 's OK , so that that that we can that we have one node per construction . Cuz even in people , like , they don't know what you 're talking about if you 're using some sort of strange construction . grad c: Well , yeah , but  , the  , that 's what the construction parser would do . grad c: if you said something completely arbitrary , it would f find the closest construction , grad b: OK . grad c: right ? But if you said something that was completel er h theoretically the construction parser would do that But if you said something for which there was no construction whatsoever , n people wouldn't have any idea what you were talking about . What do you think about that , Bhaskara ? grad a:  Well But how many constructions do could we possibly have nodes for ? grad c: In this system , or in r grad a: No , we . grad c: Oh , when p How many constructions do people have ? grad a: Yeah . grad a: Is it considered to be like in are they considered to be like very , sort of s abstract things ? grad c: Every noun is a construction . grad a: S grad c: And then , of course , the c I guess , maybe there can be the Can there be combinations of the dit grad a: Discourse - level constructions . grad c: It 's probab Yeah , I would s definitely say it 's finite . grad c: And at least in compilers , that 's all that really matters , as long as your analysis is finite . grad a: How 's that ? How it can be finite , again ? grad c: Nah , I can't think of a way it would be infinite . If the if your if your brain was totally non - deterministic , then perhaps there 's a way to get , infin an infinite number of constructions that you 'd have to worry about . grad c: So the best - case scenario would be the number of constructions or , the worst - case scenario is the number of constructions equals the number of neurons . I just Can't you use different var different levels of activation ? across ,  lots of different neurons , to specify different values ? grad b:   grad a: yeah , but there 's , like , a certain level of grad c: There 's a bandwidth issue , grad a: Bandw - Yeah , so you can't do better than something 