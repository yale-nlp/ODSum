I woke up twenty minutes ago , thinking , what did I forget ? grad d: It 's great how the br brain sort of does that . grad d: in two weeks from today ? Yeah ? More or less ? I 'll be off to Sicily and Germany for a couple , three days . grad b: Now what are y what are you doing there ? I forgot ? grad d: OK , I 'm flying to Sicily basically to drop off Simon there with his grandparents . And then I 'm flying to Germany t to go to a MOKU - Treffen which is the meeting of all the module - responsible people in SmartKom , grad b: Mmm . And then I 'm also going up to EML for a day , and then I 'm going to meet the very big boss , Wolfgang Walster , in Saarbruecken and the System system integration people in Kaiserslautern and then I 'm flying back via Sicily pick up my son come back here on the fourth of July . grad e: What a great time to be coming back to the grad b: God bless America . grad d: And I 'm sure all the the people at the airport will be happy to work on that day . grad b: Wait , aren't you flying on Lufthansa though ? grad d:   Once you get to the United States it 'll be a problem , but grad d: Yeah . And  , that 's that bit of news , and the other bit of news is we had you know , I was visited by my German project manager who A , did like what we did what we 're doing here , and B , is planning to come here either three weeks in July or three weeks in August , to actually work . grad d: And we sat around and we talked and he came up we came up with a pretty strange idea . And  , maybe it might be ultimately the most interesting thing for Eva because she has been known to complain about the fact that the stuff we do here is not weird enough . grad d: Imagine if you will , that we have a system that does all that understanding that we want it to do based on utterances . So if you have the knowledge of how to interpret " where is X ? " under given conditions , situational , user , discourse and ontological conditions , you should also be able to make that same system ask " where is X ? " grad e:   So in instead of just being able to observe phenomenon , and , guess the intention we might be able just to sort of give it an intention , and make it produce an utterance . grad b: Well , like in AI they generally do the take in , and then they also do the generation phase , like Nancy 's thing . Or  , you remember , in the the hand thing in one - eighty - two , like not only was it able to recognize but it was also to generate based upon situations . grad d: And once you 've done that what we can do is have the system ask itself . grad e: Except this smacks a little bit more of a schizophrenic computer than AI . grad d: Yeah you c if you want , you can have two parallel machines  , asking each other . What would that give us ? Would A be something completely weird and strange , and B , i if you look at all the factors , we will never observe people let 's say , in wheelchairs under you know , in under all conditions , grad e: That 's good . grad d: you know , when they say " X " , and there is a ride at the goal , and the parking is good , we can never collect enough data . If you get the system to speak to itself , you may find n break downs and errors and you may be able to learn . And  , so there 's no no end of potential things one could get out of it , if that works . grad d: So Yeah , I w See the the generation bit , making the system generate generate something , is shouldn't be too hard . grad b: I just don't think I think we 're probably a year away from getting the system to understand things . Well , if we can get it to understand one thing , like our " where is " run through we can also , maybe , e make it say , or ask " where is X ? " Or not . e I 'm sort of have the impression that getting it to say the right thing in the right circumstances is much more difficult than getting it to understand something given the circumstances and so on , you know ,  just cuz it 's sort of harder to learn to speak correctly in a foreign language , rather than learning to understand it . Right ?  grad d:  grad e: just the fact that we 'll get The point is that getting it to understand one construction doesn't mean that it will n always know exactly when it 's correct to use that construction . Right ? grad d: It 's it 's  Well , I 've I 've done generation and language production research for fo four four and a half years . And so it 's it 's you 're right , it 's not the same as the understanding . grad d: But , I think it 'd be fun to look at it , or into that question . And so that 's that 's But grad b: The basic idea I guess would be to give allow the system to have intentions , basically ? Cuz that 's basically what needs to be added to the system for it . grad d: Well , look at th eee , I think even think even What it would be the the prior intention . So let 's   , let 's say we have this grad b: Well we 'd have to seed that ,  . Right ? grad b:  grad d: What would it ask ? grad b: It wouldn't know what to ask . We 'd have to set up a situation where , it didn't know where something was and it wanted to go there . grad b: Which means that we 'd need to set up an intention inside of the system . Right ? Which is basically , " I don't know where something is and I need to go there " . grad d: Ooh , do we really need to do that ? Because , grad b: Well , no I guess not . Excel grad d: s It 's i I know it 's it 's strange , but look at it look at our Bayes - net . If we don't have Let 's assume we don't have any input from the language . Right ? So there 's also nothing we could query the ontology , but we have a certain user setting . If you just ask , what is the likelihood of that person wanting to enter some something , it 'll give you an answer . Which is , wanting to know where something is , maybe nnn and wanting I don't know what it 's gonna be , but there 's gonna be something that grad e: Well you 're not gonna are you gonna get a variety of intentions out of that then ?  , you 're just talking about like given this user , what 's the th what is it what is that user most likely to want to do ? grad d: Well you can observe some user and context stuff and ask , what 's the posterior probabilities of all of our decision nodes . grad d: You could even say , " let 's take all the priors , let 's observe nothing " , and query all the posterior probabilities . Which , if we have an algorithm that filters out whatever the the best or the most consistent answer out of that , will give us the intention ex nihilo . And that is exactly what would happen if we ask it to produce an utterance , it would be b based on that extension , ex nihilo , which we don't know what it is , but it 's there . So we wouldn't even have to t to kick start it by giving it a certain intention or observing anything on the decision node . And whatever that maybe that would lead to " what is the castle ? " , grad b: I 'm just grad d: or " what is that whatever " . grad b: I guess what I 'm afraid of is if we don't , you know , set up a situation , we 'll just get a bunch of garbage out , like you know , everything 's exactly thirty percent . So what we actually then need to do is is write a little script that changes all the settings , you know , go goes through all the permutations , which is we did a didn't we calculate that once ? grad b: Well that was that was absurdly low , in the last meeting , grad d: It 's a grad c: grad b: cuz I went and looked at it cuz I was thinking , that could not be right , and it would it was on the order of twenty output nodes and something like twenty grad c: And like thirty input nodes grad b: thirty input nodes . grad c: or some grad b: So to test every output node , would at least Let 's see , so it would be two to the thirty for every output node ? Which is very th very large . I 'm talking about billions and billions and billions and a number two to the thirty is like a Bhaskara said , we had calculated out and Bhaskara believes that it 's larger than the number of particles in the universe . That 's just That 's  It 's a billion , right ? grad b: Two to the thirty ? Well , two to the thirty is a billion , but if we have to do it two to the twenty times , then that 's a very very large number . grad b: Cuz you have to query the node , for every a  , or query the net two to the twenty times . grad e: Yeah , it 's g Anyway , the point is that given all of these different factors , it 's  e it 's it 's still going to be impossible to run through all of the possible situations or whatever . grad b: If it takes us a second to do , for each one , and let 's say it 's twenty billion , then that 's twenty billion seconds , which is grad e: Yeah . grad e: Long ! grad c:  grad b: Hours and hours and hours and hours . grad e: Tah - dah ! grad b: Which probabilistically will be good enough . So , it be it it 's an idea that one could n for for example run run past , what 's that guy 's name ? You know ? He - he 's usually here . We we we we g grad b: Wait , who ? grad e: Yeah , i that would the g the bald guy . And  , what other news do I have ? Well we fixed some more things from the SmartKom system , but that 's not really of general interest , Oh ! Questions , yeah . How is the generation XML thing ? grad b: I 'm gonna work on that today and tomorrow . I 've tried about five times so far , where I work for a while and then I 'm like , I 'm hungry . I found everything that I need and stu and  , grad d: But st grad b: At the b  furthermore , I told Jerry that I was gonna finish it before he got back . He 's coming back when ?  next grad b: Well , I think we think we 'll see him definitely on Tuesday for the next Or , no , wait . grad b: I think I will try to work on the SmartKom stuff and I 'll if I can finish it today , I 'll help you with that tomorrow , if you work on it ? I don't have a problem with us working on it though ? So . grad b:  we just  it wouldn't hurt to write up a paper , cuz then , yeah I was talking with Nancy and Nancy said , you don't know whether you have a paper to write up until you write it up . grad d: Well grad b: And since Jerry 's coming back , we can run it by him too .  , what 's your input ? grad e: Well , I don't have much experience with  , conference papers for compu in the computer science realm , and so when I looked at what you had , which was apparently a complete submission , I just sort of said what just I I didn't really know what to do with it , like , this is the sort of the basic outline of the system or whatever , or or " here 's an idea " , right ? That 's what that paper was , " here 's here 's one possible thing you could do " , grad d:   grad e: short , eight pages , and I just don't know what you have in mind for expanding . Like I 'd I what I didn't do is go to the web site of the conference and look at what they 're looking for or whatever . Well , it seems to me that  grad b: Wait , is this a computer science conference or is it a grad d: well it 's more It 's both , right ? It 's it 's sort of t cognitive , neural , psycho , linguistic , but all for the sake of doing computer science . So it 's sort of cognitive , psycho , neural , plausibly motivated , architectures of natural language processing . So it seems pretty interdisciplinary , and  , w w the keynote speaker is Tomasello and blah - blah - blah , grad e: Right . grad d: so , W the the question is what could we actually do and and and keep a straight face while doing it . grad d: well , you can say we have done a little bit and that 's this , and  sort of the rest is position paper , " we wanna also do that " . Might be more interesting to do something like let 's assume  , we 're right , we have as Jerry calls it , a delusion of adequacy , and take a " where is X " sentence , grad e:   grad d: and say , " we will just talk about this , and how we cognitively , neurally , psycho - linguistically , construction grammar - ally , motivated , envision  , understanding that " . That should be able to we should be able to come up with , you know , a sort of a a parse . There 's a s diagram somewhere which tells you how to put that grad a: I know , I didn't understand that either ! grad b: No wait . grad b: See the p how the plastic things ar arch out like that ? There we go . grad a: It does ! I 'm sorry I didn't mean to grad e: But that 's what you get for coming late to the meeting . grad e: grad d: We 're talking about this  , alleged paper that we may , just , sort of w grad a: Oh ! Which Johno mentioned to me . And I just sort of brought forth the idea that we take a sentence , " Where is the Powder - Tower " , grad a:   grad d: and we we p pretend to parse it , we pretend to understand it , and we write about it . About how all of these things grad a: What 's the part that 's not pretend ? The writing ? grad d: OK , then we pretend to write about . grad a: Tha - Which conference is it for ? grad d: It 's the whatever , architectures , eh you know , where There is this conference , it 's the seventh already international conference , on neu neurally , cognitively , motivated , architectures of natural language processing . grad d: And the keynote speakers are Tomasello , MacWhinney ? grad a: Whinney . grad a: is is it normally like like , dialogue systems , or , you know , other NLP - ish things ? grad d: No no no no no no no no . grad e: Why , we 've got over a week ! grad d: It would be nice to go write two papers actually . And one one from your perspective , and one from our peve per per grad a: th that 's the kinda thing that maybe like , the general  con sort of like NTL - ish like , whatever , the previous simulation based pers maybe you 're talking about the same kind of thing . grad d: Well , I I also think that if we sort of write about what we have done in the past six months , we we we could sort of craft a nice little paper that if it gets rejected , which could happen , doesn't hurt grad a:   grad d: because it 's something we eh grad a: Having it is still a good thing . grad b: Will I ? grad a: When is it and where ? grad d: In case of grad e:  ! grad d: It 's on the twenty second of September , in Saarbruecken Germany . So , is the What Are you just talking about you know , the details of how to do it , or whether to do it , or what it would be ? grad e: What would one possibly put in such a paper ? grad d: What to write about . grad a: Or what to write about ? grad d: What is our what 's our take home message . What what do we actually Because  , it I don't like papers where you just talk about what you plan to do .  , it 's obvious that we can't do any kind of evaluation , and have no you know , we can't write an ACL type paper where we say , " OK , we 've done this grad a:   And  , maybe even That 's maybe the time to introduce the the new formalism that you guys have cooked up . grad b: But that grad e: Are in the process of grad a: How many pages ? grad b: don't they need to finish the formalism ? grad d: It 's just like four pages . grad a: Four pages ? grad d:  it 's it 's not even a h grad e: Yeah . grad d: I don't know w Did you look at it ? Yeah , it depends on the format . Oh , I thought you were I thought we were talking about something which was much more like ten or something . grad a: And it 's also difficult to even if you had a lot of substance , it 's hard to demonstrate that in four pages , basically . grad a:  it 's still it 's still grad d: Well I  maybe it 's just four thousand lines . I do I don't They don't want any They don't have a TeX f style @ @ guide . Why , for whatever reason , grad a: Not including figures and such ? grad d: I don't know . Well , grad d: We 'll just  grad b: I would say that 's closer to six pages actually . Isn't a isn't it about fifty s fifty five , sixty lines to a page ? grad d: I d don't quote me on this . This is numbers I I have from looking o grad b: How many characters are on a line ? grad d: OK . grad a: ASCII ? grad d: Let 's let 's wh wh what should we should should we  , discuss this over tea and all of us look at the web ? Oh , I can't . grad a: Wha - w grad d: Look at the web page and let 's talk about it maybe tomorrow afternoon ? grad a: More cues for us to find it are like , neural cons grad d: Johno will send you a link . grad d: And I 'm also flying grad e: I got this from the two one two . Yes ? grad d: I 'm flying to Sicily next in a w two weeks from now , grad a: Oh , lucky you . And otherwise you haven't missed much , except for a really weird idea , but you 'll hear about that soon enough . grad a: The idea that you and I already know about ? That you already told me ? Not that OK . Yeah , that is something for the rest of the gang to to g grad e: The thing with the goats and the helicopters ? grad d: Change the watchband . Did you catch that allusion ? It 's time to walk the sheep ? grad e: No . grad d: It 's a a  presumably one of the Watergate codes they  grad e: Oh . grad d: Anyways , th  , don't make any plans for spring break next year . grad a: Does that mean Does that mean you 'll get you 'll fly us there ? grad e: We 'll see . grad d: But coconut anana pineapple , that 's that 's tricky , yeah . grad e: So , but we have to decide what , like , sort of the general idea of grad b: Potatoes . Sorry ! grad e: we 're gonna have an example case  , right ? I m the the point is to like this " where is " case , or something . grad d: Yeah , maybe you have It would be kind of The paper ha would have , in my vision , a nice flow if we could say , well here is th the th here is parsing if you wanna do it c right , here is understanding if you wanna do it right , and you know without going into technical grad e:   grad a: But then in the end we 're not doing like those things right yet , right ? Would that be clear in the paper or not ? grad d: That would be clear , we would grad a: OK . grad d: I I mailed around a little paper that I have grad a: It would be like , this is the idea . Oh , I didn't get that , grad d: w we could sort of say , this is grad a: did I ? Oops . grad d: See this , if you if you 're not around , and don't partake in the discussions , and you don't get any email , grad a: I 'm sorry . grad d: Su So we could we could say this is what what 's sort of state of the art today . And grad a: And how much to get into the cognitive neural part ? grad b: That 's the only That 's the question mark . grad d: We grad b: Don't you need to reduce it if it 's a or reduce it , if it 's a cognitive neuro grad a: Well , you don't have t  the conference may be cognitive neural , doesn't mean that every paper has to be both . grad d: Yeah , and you can you can just point to the to the literature , grad e: Mmm . grad d: you can say that construction - based You know grad a: So i so this paper wouldn't particularly deal with that side although it could reference the NTL - ish sort of , like , approach . grad a: The fact that the methods here are all compatible with or designed to be compatible with whatever , neurological neuro neuro - biol su stuff . grad a: Yeah , I guess four pages you could  you could definitely it 's definitely possible to do it . Like introducing the formalism might be not really possible in detail , but you can use an example of it . grad e: Well , l looking at yeah , looking at that paper that that you had ,  you know , like , you didn't really explain in detail what was going on in the XML cases or whatever you just sorta said well , you know , here 's the general idea , some stuff gets put in there . You know , hopefully you can you can say something like constituents tells you what the construction is made out of , you know , without going into this intense detail . grad e: Give them the one paragraph whirlwind tour of w w what this is for , grad a: Yeah . So this will be sort of documenting what we think , and documenting what we have in terms of the Bayes - net stuff . grad d: And since there 's never a bad idea to document things , no ? grad a: That 's th that 's definitely a good idea . grad d: That would be my ,  We we should sketch out the details maybe tomorrow afternoon - ish , if everyone is around . grad d: And  , the  , other thing , yeah we actually Have we made any progress on what we decided , last week ? I 'm sure you read the transcript of last week 's meeting in red so sh so you 're up to dated caught up . grad d: We decided t that we 're gonna take a " where is something " question , and pretend we have parsed it , and see what we could possibly hope to observe on the discourse side . grad b: Remember I came in and I started asking you about how we were sor going to sort out the  , decision nodes ? grad a: Yes ! What 'd you say ? grad b: I remember you talking to me , just not what you said . grad b: Well , there was like we needed to or  , in my opinion we need to design a Bayes another sub - Bayes - net You know , it was whether it was whether we would have a Bayes - net on the output and on the input , grad a: Oh . grad b: or whether the construction was gonna be in the Bayes - net , grad a: Oh , yeah . grad b: and grad a: So that was was that the question ? Was that what grad b: Well that was related to what we were talking about . grad d: Should I introduce it as SUDO - square ? grad b: Yeah sure . The SUDO - square is , " Situation " , " User " , " Discourse " , right ? " Ontology " . grad e: Oh I saw the diagram in the office , grad a: Oh my god , that 's amazing ! grad d: Mmm . grad e: Way ! grad d: Is it ? grad a: Someone 's gonna start making Phil Collins jokes . grad e: You know like " Sussudio " , grad a: Yeah , come on . grad e: in here grad d: Oh Well , also he 's talking about suicide , and that 's that 's not a notion I wanna have evoked . I didn't really listen to it , grad d: The grad a: I was too young . So , what 's going on here ? So what are what grad d: So , grad e: Was wollte der Kuenstler uns damit sagen ? grad a: Stop excluding me . grad d: OK , so we have tons of little things here , grad a: I can't believe that that 's never been thought of before . grad d: and we 've grad b: Wait , what are the dots ? I don't remember what the dots were . grad a:  grad d: You know , these are our , whatever , belief - net decision nodes , and they all contribute to these things down here . grad a: Wait , wait , what 's the middle thing ? grad d: That 's EDU . grad e: That 's a c grad d: e e Our e e e grad a: But wh  grad e: That 's grad d: You . grad a: But what is it ? grad d: Well , in the moment it 's a Bayes - net . Eh I have taken care that we actually can build little interfaces , to other modules that will tell us whether the user likes these things and , n the or these things , and he whether he 's in a wheelchair or not , grad a: OK . Is that supposed to be the international sign for interface ? grad d: I think so , yeah . grad d: No , this is a RME core by agent design , I don't know . grad d: There 's maybe a different grad e: So wait , what a what are these letters again , Situr - Situation , User , Discourse and grad d: Situation , user , d ontology . grad d: And for example w i s I Irena Gurevich is going to be here eh , end of July . grad d: So , we have discussed in terms of the EVA grad a: Grateful for us ? grad d:  grad a: Did you just say grateful for us ? OK , sorry . grad d: Think of back at the EVA vector , and Johno coming up with the idea that if the person discussed the discussed the admission fee , in eh previously , that might be a good indication that , " how do I get to the castle ? " , actually he wants to enter . grad d: Or , you know , " how do I get to X ? " discussing the admission fee in the previous utterance , is a good indication . grad a:  grad d: So we don't want a hard code , a set of lexemes , or things , that person 's you know , sort of filter , or  search the discourse history . grad d: So what would be kind of cool is that if we encounter concepts that are castle , tower , bank , hotel , we run it through the ontology , and the ontology tells us it has  , admission , opening times , it has admission fees , it has this , it has that , and then we we we make a thesaurus lexicon , look up , and then search dynamically through the  , discourse history for occurrences of these things in a given window of utterances . grad d: And that might , you know , give us additional input to belief A versus B . grad a: So it 's not just a particular word 's OK , so the you 're looking for a few keys that you know are cues to sorry , a few specific cues to some intention . grad e: so , wait so  , since this since this sort of technical stuff is going over my head , grad b: And then grep , basically . grad e: the the point is that you  that when someone 's talking about a castle , you know that it 's the sort of thing that people are likely to wanna go into ? Or , is it the fact that if there 's an admission fee , then one of the things we know about admission fees is that you pay them in order to go in ? And then the idea of entering is active in the discourse or something ? And then grad d: Well grad e: blah - blah - blah ? grad d: the the idea is even more general . grad d: The idea is to say , we encounter a certain entity in a in a in a utterance . So le let 's look up everything we the ontology gives us about that entity , what stuff it does , what roles it has , what parts , whatever it has . And , then we look in the discourse , whether any of that , or any surface structure corresponding to these roles , functions aaa has ever occurred . grad d: And then , the discourse history can t tell us , " yeah " , or " no " . grad d: So , we may think that if you say  , " where is the theater " , whether or not he has talked about tickets before , then we he 's probably wanna go there to see something . grad d: Or " where is the opera in Par - Paris ? , grad e: OK . grad d: yeah ? Lots of people go to the opera to take pictures of it and to look at it , grad e:   grad d: And , the discourse can maybe tell us w what 's more likely if we know what to look for in previous statements . And so we can hard code " for opera , look for tickets , look for this , look for that , grad e: OK . grad d: or look for Mozart , look for thi " but the smarter way is to go via the ontology and dynamically , then look up u stuff . But you 're still doing look up so that when the person So the point is that when the person says , " where is it ? " then you sort of say , let 's go back and look at other things and then decide , rather than the other possibility which is that all through discourse as they talk about different things You know like w prior to the " where is it " question they say , you know , " how much does it cost to get in , you know , to to see a movie around here " , " where is the closest theater " The the the point is that by mentioning admission fees , that just sort of stays active now . grad e: And then , over in your Bayes - net or whatever , when when the person says " where is it " , you 've already got , you know since they were talking about admission , and that evokes the idea of entering , then when they go and ask " where is it " , then you 're Enter node is already active grad d:   grad e:  that 's the sort of cognitive linguistic - y way , grad d: Yeah , e ultimately that 's also what we wanna get at . So , of course we have to keep memory of what was the last intention , and how does it fit to this , and what does it tell us , in terms of of the the what we 're examining . grad d: And furthermore ,  we can idealize that , you know , people don't change topics , grad e:   But , even th for that , there is a student of ours who 's doing a dialogue act  , recognition module . grad d: So , maybe , we 're even in a position where we can take your approach , which is of course much better , as to say how how do these pieces grad e: Mmm . So these are issues but we what we actually decided last week , is to , and this is , again , for your benefit is to  , pretend we have observed and parsed an utterance such as " where is the Powder - Tower " , or " where is the zoo " , and specify  , what what we think the the output  , observe , out i input nodes for our Bayes - nets for the sub sub - D , for the discourse bit , should be . So that And I will I will then come up with the ontology side  , bits and pieces , so that we can say , OK we we always just look at this utterance . That 's the only utterance we can do , it 's hard coded , like Srini , sort of hand parsed , hand crafted , but this is what we hope to be able to observe in general from utterances , and from ontologies , and then we can sort of fiddle with these things to see what it actually produces , in terms of output . grad e:  grad d: So we need to find out what the " where is X " construction will give us in terms of semantics and Simspec type things . We decided sort of the the prototypical " where is X " , where you know , we don't really know , does he wanna go there , or just wanna know where it is . grad e: Well we were grad d: So the difference of " where is the railway station " , versus where where " where is Greenland " . grad b:  ah grad e: So , we 're supposed to  we 're talking about sort of anything that has the semantics of request for location , right ? actually ? Or , anyway , the node in the  the ultimate , in in the Bayes - net thing when you 're done , the the node that we 're talking about  , is one that says " request for location , true " , or something like that , right ?  , and and exactly how that gets activated , you know , like whether we want the sentence " how do I get there ? " to activate that node or not , you know , that 's that 's sort of the issue that sort of the linguistic - y side has to deal with , right ? grad d: Yeah , but it Yea - Nnn Well actually more m more the other way around . We wanted something that represents uncertainty  we in terms of going there or just wanting to know where it is , for example . grad d: And so this is prototypically @ @ found in the " where is something " question , surface structure , grad e: OK . grad b: We grad d: which can be p you know , should be maps to something that activates both . grad b: I don't see unde how we would be able to distinguish between the two intentions just from the g utterance , though . grad d: The grad b:  bef or , before we don't before we cranked it through the Bayes - net . grad b: OK , but then so basically it 's just a for every construction we have a node in the net , right ? And we turn on that node . What is the  Well grad b: And then given that we know that the construction has these two things , we can set up probabilities we can s basically define all the tables for ev for those grad d: Yeah , it should be So we have  , i let 's assume we we call something like a loc - X node and a path - X node . And what we actually get if we just look at the discourse , " where is X " should activate or should grad e: Mmm . Should be both , whereas maybe " where is X located " , we find from the data , is always just asked when the person wants to know where it is , and " how do I get to " is always asked when the person just wants to know how to get there . Right ? So we want to sort of come up with what gets  , input , and how inter in case of a " where is " question . So what what would the outcome of of your parser look like ? And , what other discourse information from the discourse history could we hope to get , squeeze out of that utterance ? So define the the input into the Bayes - net based on what the utterance , " where is X " , gives us . So definitely have an Entity node here which is activated via the ontology , grad a: s grad d: so " where is X " produces something that is s stands for X , whether it 's castle , bank , restroom , toilet , whatever . And then the ontology will tell us grad a: That it has a location or something like that ? or th the ontology will tell us where actually it is located ? grad d: No . grad d: Where it is located , we have , a user proximity node here somewhere , grad a: OK . grad d: e which tells us how far the user how far away the user is in respect to that  entity . So you 're talking about , for instance , the construction obviously involves this entity or refers refers to this entity , grad d:   grad a: and from the construction also you know that it is a location is or a thing thing that can be located . Sh - and that 's the thing that is being that is the content of the question that 's being queried by one interpretation of " where is X " . So is the question  it 's just that I 'm not sure what the Is the question , for this particular construction how we specify that that 's the information it provides ? Or or asked for ? b Both sides , right ? grad d: Yeah , you don't need to even do that . grad a: Observed when you heard the speaker say " where is X " , or when when that 's been parsed ? grad d:   grad b: I d I just I don't like having characterizing the constructions with location and path , or li characterizing them like that . Cuz you don't It seems like in the general case you wouldn't know how how to characterize them . There could be an interpretation that we don't have a node for in the  it just seems like @ @ has to have  a node for the construction and then let the chips fall where they may . And , in this cas and since since it can mean either of those things , it would light both of those up . grad e:  grad d: So I think r in here we have " I 'll go there " , right ? grad b: Answers ? grad d: And we have our Info - on . So in my c my case , this would sort of make this happy , and this would make the Go - there happy . What you 're saying is we have a Where - X question , Where - X node , that makes both happy . Right ? That 's what you 're proposing , which is , in my mind just as fine . So w if we have a construction node , " where is X " , it 's gonna both get the po posterior probability that it 's Info - on up , grad b: Mmm , yeah . grad d: Info - on is True - up , and that Go - there is True - up , as well . Which would be exactly analogous to what I 'm proposing is , this makes  makes something here true , and this makes something also something here true , and this makes this True - up , and this makes this True - up as well . You know with with this points to this points to that , and so on because I don't know , it grad a: Is -  , grad d: Yeah , because we get we get tons of constructions I think . Because , you know , mmm people have many ways of asking for the same thing , grad e: Yeah . grad d: and grad a: So un grad b: I change I changed my mind actually . grad a: I have a different kinda question , might be related , which is , OK so implicitly everything in EDU , we 're always inferring the speaker intent , right ? Like , what they want either , the information that they want , or It 's always information that they want probably , of some kind . Right ? Or I I don't know , or what 's something that they grad d: The system doesn't massage you , no . So I don't know if the  i if th just there 's more s here that 's not shown that you it 's already like part of the system whatever , but , " where is X " , like , the fact that it is , you know , a speech - act , whatever , it is a question . It 's a question that , queries on some particular thing X , and X is that location . grad a: So that seems different from just having the node " location - X " and that goes into EDU , right ? grad d: Yeah . That 's that 's grad a: So tha is that what you 're t talking about ? grad d: So , w Exactly . grad d: The next one would be what we can squeeze out of the  I don't know , maybe we wanna observe the  ,  the length of of the words used , and , or the prosody grad a: Mmm . So in some ways grad d: I don't know , grad a: so in some ways in the other sort of parallel set of mo more linguistic meetings we 've been talking about possible semantics of some construction . grad a: Right ? Where it was the simulation that 's , according to it you know , that that corresponds to it , and as well the as discourse , whatever , conte infor in discourse information , grad d:   So , are we looking for a sort of abbreviation of that , that 's tailored to this problem ? Cuz that that has , you know , basically , you know , s it 's in progress still it 's in development still , but it definitely has various feature slots , attributes , bindings between things grad d:   U that 's exactly r  , why I 'm proposing It 's too early to have to think of them of all of these discourse things that one could possibly observe , grad a:  - huh . grad d: so let 's just assume grad a: For the subset of grad d: human beings are not allowed to ask anything but " where is X " . That exactly " where is X " , grad d: In ter grad a: not the the choices of " where is X " or " how do I get to X " . grad d: And , but you know , do it do it in such a way that we know that people can also say , " is the town hall in front of the bank " , so that we need something like a w WH focus . Nuh ? Should be should be there , that , you know , this the whatever we get from the grad a: Wait , so do , or do not take other kinds of constructions into account ? grad d: Well , if you if you can , oh definitely do , grad a: OK . Right ? If i if if it 's not at all triggered by our thing , then it 's irrelevant , grad a: it seems like for instance , " where is X " , the fact that it might mean  , " tell me how to get to X " , like Do y So , would you wanna say that those two are both , like Those are the two interpretations , right ? the the ones that are location or path . So , you could say that the s construction is a question asking about this location , and then you can additionally infer , if they 're asking about the location , it 's because they wanna go to that place , in which case , the you 're jumping a step step and saying , " oh , I know where it is grad d:   grad a: but I also know how to get they wanna seem they seem to wanna get there so I 'm gonna tell them " . So there 's like structure grad e: Right , th this it 's not it 's not that this is sort of like semantically ambiguous between these two . grad e: It 's really about this but why would you care about this ? Well , it 's because you also want to know this , or something like that right ? grad a: So it 's like you infer the speaker intent , grad d:   grad a: and then infer a plan , a larger plan from that , for which you have the additional information , grad e: Yeah . grad d: If you think about , focus on this question , how would you design that ? grad e:   grad d: Is it do you feel confident about saying this is part of the language already to to detect those plans , and why would anyone care about location , if not , you know and so forth . grad d: Or do you actually ,  this is perfectly legitimate , and I I would not have any problems with erasing this and say , that 's all we can activate , based on the utterance out of context . grad a: like , grad d: And then the the the miracle that we get out the intention , Go - there , happens , based on what we know about that entity , about the user , about his various beliefs , goals , desires , blah - blah - blah . But this is the sort of thing , I I propose that we think about , grad a: OK . grad d: so that we actually end up with  , nodes for the discourse and ontology so that we can put them into our Bayes - net , never change them , so we all there is is " where is X " , and , Eva can play around with the observed things , and we can run our better JavaBayes , and have it produce some output . And for the first time in th in in the world , we look at our output , and  and see  whether it it 's any good . grad d: Yeah , I  , for me this is just a ba matter of curiosity , I wanna would like to look at  , what this ad - hoc process of designing a belief - net would actually produce . And , maybe it also h enables you to think about certain things more specifically , come up with interesting questions , to which you can find interesting answers . grad d: So th this might be a nice opening paragraph for the paper as saying , " you know people look at kinds of at ambiguities " , and  , in the literature there 's " bank " and whatever kinds of garden path phenomenon . A , A ,  these things are never really ambiguous in discourse , B , B , don't ever occur really in discourse , but normal statements that seem completely unambiguous , such as " where is the blah - blah " , actually are terribly complex , and completely ambiguous . grad d: And so , what every everybody else has been doing so far in in in you know , has been completely nonsensical , and can all go into the wastepaper bin , and the only grad e: That 's always a good way to begin . grad d: Nice overture , but , you know , just not really OK , I 'm eja exaggerating , but that might be , you know , saying " hey " , you know , some stuff is is actually complex , if you look at it in in in the vacuum grad e:   And some stuff that 's as that 's absolutely straightforward in the vacuum , is actually terribly complex in reality . grad b: When do you need to start wizarding ? grad d: At four ten . grad d: Also we 're getting a a person who just got fired  , from her job . Which is good news in the sense that if we want to continue , after the thir thir after July , we can . And ,  and that 's also maybe interesting for Keith and whoever , if you wanna get some more stuff into the data collection . grad d: Look at the results we 've gotten so far for the first , whatever , fifty some subjects ? grad a: Fifty ? You 've had fifty so far , or ? grad d: No , we 're approaching twenty now . grad d: But , until Fey is leaving , we surely will hit the some of the higher numbers . Is that around ? Like , cuz that 's pretty much getting posted or something right away when you get it ? grad d:  . grad e: Or ? I guess it has to be transcribed , huh ? grad d: We have  , eh found someone here who 's hand st hand transcribing the first twelve . And I can ch ch st e grad e: You know  you know that I that I looked at the first the first one and got enough data to keep me going for , you know , probably most of July . grad d: But you can listen to a y y y You can listen to all of them from your Solaris box 